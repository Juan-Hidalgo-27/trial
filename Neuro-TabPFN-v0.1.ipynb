{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a071cb1-8c29-4535-a208-2ae0b51d5b45",
   "metadata": {},
   "source": [
    "# Neuro-TabPFN (Debug Mini-Pipeline)\n",
    " - ISLES'24 -> VAE (Z) -> Mock InterSynth -> TabICL/Do-PFN\n",
    " - Objetivo causal: CATE  ≜  E[ Y|do(T=1),Z=z] - E[ Y|do(T=0),Z=z]\n",
    "\n",
    "Este notebook implementa el pipeline completo para predecir outcomes de pacientes con stroke usando:\n",
    "\n",
    "1. **ISLES'24**: Dataset de imágenes cerebrales 3D (máscaras de lesiones)\n",
    "2. **VAE**: Comprime las imágenes 3D a vectores latentes\n",
    "3. **TabPFN**: Predice outcomes clínicos\n",
    "   \n",
    "Orden: Config → Carga/EDA → VAE → Latentes → Mock InterSynth → TabPFN/TabICL/Do-Loss → Métricas → Guardado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cd9649-cb03-4ab9-b0f3-6f3ce07ea6fe",
   "metadata": {},
   "source": [
    "## PARTE 0: Set up inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59419553-51b9-4043-8987-a3554ad8f685",
   "metadata": {},
   "source": [
    "### Instalación de Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1095d141-2c23-4872-b880-992525d3fd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\debugging\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = r\"D:\\hf-datasets-cache\" \n",
    "\n",
    "ROOT = \"H:/My Drive/Debbuging Neuro\"\n",
    "\n",
    "os.chdir(ROOT)\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0772a75e-29cf-4631-b2a3-10229b3a11bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5920e6b0-7828-4083-aef5-03e0f6a14020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MONAI spacing-aware transforms\n",
    "from monai.transforms import Spacing\n",
    "from monai.data import MetaTensor\n",
    "\n",
    "from scipy.ndimage import zoom\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fec50b-3866-45d2-a9bd-13ec1d5fc336",
   "metadata": {},
   "source": [
    "### Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c26b7f6b-583e-49bc-a93c-b53a67f08b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuración del pipeline\"\"\"\n",
    "    SEED = 42\n",
    "    DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "    # Datos\n",
    "    HF_DATASET = \"hugging-science/isles24-stroke\"\n",
    "    N_CASES = 149\n",
    "    TARGET_SHAPE = (96, 112, 96)  # Matches MNI aspect ratio\n",
    "    ALLEN_ATLAS_PATH = r\"ROOT\\Data\\atlases\\allen_receptor_density.nii.gz\"\n",
    "    TARGET_SPACING = (2.0, 2.0, 2.0)  # 2mm isotropic (standard MNI resolution)\n",
    "\n",
    "    # VAE\n",
    "    LATENT_DIM = 50\n",
    "    VAE_EPOCHS = 30\n",
    "    VAE_BATCH_SIZE = 4\n",
    "    VAE_LR = 1e-4\n",
    "\n",
    "    # SCM / causal\n",
    "    SCM_EFFECT = 2.0    # Efecto causal verdadero (Ground Truth)\n",
    "    SCM_WEIGHT_Z = 1.5  # Fuerza del confundidor\n",
    "\n",
    "\n",
    "    # TabICL\n",
    "    D_MODEL = 128\n",
    "    N_LAYERS = 4\n",
    "    N_HEAD = 4\n",
    "    LR_TABICL = 1e-3\n",
    "    SYN_BATCH = 8    # Batches de \"historias\" sintéticas\n",
    "    SYN_SEQ = 64     # Pacientes por historia\n",
    "\n",
    "cfg = Config()    \n",
    "\n",
    "# Reproducibilidad\n",
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "        \n",
    "set_seeds(cfg.SEED)\n",
    "print(f\"Device: {cfg.DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca1de1f-6b73-457b-8acc-07397aeba8d0",
   "metadata": {},
   "source": [
    "# PARTE 0: ISLES DATASET INSPECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f05c92d0-83a7-4fc1-a78f-aa49a79ae26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading ISLES'24 stream...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18950a00fabb425787a54486151e1537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " INSPECTING FIRST 3 SAMPLES\n",
      "------------------------------------------------------------\n",
      " sub-stroke0001\n",
      "    Shape: (512, 595, 75)\n",
      "    Values: [0. 1.] (Should be 0. and 1. for masks)\n",
      "    Affine/Spacing Info: [[-4.68750000e-01 -0.00000000e+00  0.00000000e+00  1.20000000e+02]\n",
      " [-0.00000000e+00  4.52777714e-01 -5.17637968e-01 -2.16447571e+02]\n",
      " [ 0.00000000e+00  1.21321395e-01  1.93185163e+00 -2.01136841e+02]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      " sub-stroke0002\n",
      "    Shape: (512, 605, 75)\n",
      "    Values: [0. 1.] (Should be 0. and 1. for masks)\n",
      "    Affine/Spacing Info: [[-5.41015625e-01  0.00000000e+00  0.00000000e+00  1.26588997e+02]\n",
      " [ 0.00000000e+00  5.11540321e-01 -6.51136330e-01 -2.26156067e+02]\n",
      " [ 0.00000000e+00  1.76137464e-01  1.89103714e+00 -2.33005447e+02]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      " sub-stroke0003\n",
      "    Shape: (512, 633, 70)\n",
      "    Values: [0. 1.] (Should be 0. and 1. for masks)\n",
      "    Affine/Spacing Info: [[-4.10156250e-01 -0.00000000e+00  0.00000000e+00  9.70199966e+01]\n",
      " [-0.00000000e+00  3.85420799e-01 -6.84040189e-01 -1.72771271e+02]\n",
      " [ 0.00000000e+00  1.40281677e-01  1.87938523e+00 -1.92425629e+02]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD7CAYAAABOrvnfAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAFeNJREFUeJztnQm4TVUbx7dKc2me5wGZ5zmSCMlUN6VSURlSZAhFE6KRR8OTJLNSplQSDURSikYUETJVbmTIvL/nt77WefY599x7D66K9/97nuO6+66799r7rv9a73rXet+dKwzDMBBCmOCgf7sCQoh/DgleCENI8EIYQoIXwhASvBCGkOCFMIQEL4QhJHghDCHBC2EICT6H+Oyzz4J8+fIFr776arCv2b59e7Bq1aocPefll18eXHfddTl2vi1btgRPPfVUULVq1aBo0aJBo0aNgk8//TRDuZ07dwYDBgwIatSoERQpUiSoW7duMHHixKTnHD16dFCnTh13viuvvDIYMWJEhjJsHH3jjTeCq6++2pWrVq1a8Nxzz7lnJiT4/Y4VK1a4xjx16tTgv0z79u2DV155xQmuU6dOTnC333578MUXX8SVe/zxx13HUKJEieD+++8PTjjhhODee+8N3n777bhyQ4YMCR544IHg7LPPDjp37hzkz58/ePTRR4P+/fvHleP7rl27BmeeeaYrV7FixeD5558POnTo8I/c938e9tKLvWfWrFlh3rx5w5EjR+6X16latWqYlpaWI+eaOXOmq+OgQYNixzZt2hRWq1YtbNCgQezYkiVLwvz584fdu3ePHduxY0fYqFGjsGLFiuHWrVvdsfXr14fFihULW7ZsGe7atStWtm3btmGRIkXCtWvXxsoVLFgwbNasWVx9evfu7eozb9680Doy6UWO89ZbbwW5c+eOmyIceeSRwbXXXht8//33wc8//+yOvfPOO8GuXbuCG2+8MVbu4IMPdt//9ttvwezZs92xDz/8MNi8eXPQuHHjIFeuXLGyN998s5s6vP/+++77X375JShUqJArF6VMmTLu64IFC8z/tc0LfuPGjc5UZK5JY+ErpuIff/zhGgdmIXPzrVu3xjUWzFCO08ii/PXXX8GDDz4YlCpVKihZsmRwzz33BMuXL8+RuowdOzZo0qSJ+//DDz/sru+P8/93333XzW0LFy7s6gAICnP4qquucucsX7580LFjRzc1yIoNGzYEDRs2DIoVK+b8E57Fixe7e0JEzLkpkzjn/u6774Lzzz/fiTxKwYIFYz/3X48++mhXNrtyQP2zKlegQIHgtddec/6IKPPnz3dfTz/99MA6hwTGadu2rZtXIiTmhwsXLnTOoB9++CGpUyg7nn322eC0004L7rrrLidUxDZnzpxgwoQJbn66N3UpXbp00KJFi+DFF190QitXrlzc79NZ4Bzj+uecc447hriZD1epUsWNfCtXrgxGjhwZzJgxI3j99dfddRKh02revHnw008/uWuVLVvWHac+N9xwQ3DssccGzZo1C4444ohgypQpbs7966+/Brfeeqsrt2bNGtcZJHLKKae4r9TBlzv11FOzLce5Dz/88OC4446LK3fYYYe5Y75clB07drhO7YMPPgheeOEF10GV/fs+LGNa8Onp6cH06dOdCdmuXbvYcUamjz/+2I1yu8sxxxzjvMSMXMAof+eddzpPNM6rvakL4qxQoYITIYKqV69e3DkY2aLXQNSIPS0tLejRo0fsOB5xOobevXs7h1aUbdu2Ba1bt3ajJkLBIvB0797d3df48eOd6L1ZzYj/zDPPOA87ndqmTZtcZ5AIovUdClDuqKOOSqmcP5YIovflojAd8B3QiSee6DrDXJHpgFVMm/Q0Xj6YpJjFf/75Z2yk5XvEu7tcf/31MbEDI+t5550XfPTRR/u8Lokjvp/btmzZMu44y1V4r+lIEHh0iQxvNh0F3vNKlSrFfoa18vnnnweVK1d2oycdFB+O04Ew5fnkk0+CVIgKLysR+p+x1JZKuSiY71hbjzzyiOs06eA+i0xNrGJa8IceeqgbtWisXbp0caMZZi/LSevWrdujc15wwQUZjjEy+zkzIzUOqegHx1NO1OWkk06K+x7/AiMjS1SJXHjhhU7smMseRnVMdPjyyy/jyuOHQHijRo1ydYt+/JKXN60RGPeUiD/mO8S9LQc8r2gH66GTpSOiA2ZvBE7EJ598MrCOaZMeateuHVx66aVurseIN3PmTDe6ITRG1sxgNExGZiPRIYf8/1H37NkzGDduXNzPevXq5ebk2dXFz20z46CD4vtvBOo/ifXCmQcIIVrHp59+2tUPnwEmup+L+/tlpKxZs2bS63t/wBlnnOE6skR85+Ln7ZTznvjsymG249SMihux0xlm91xOPvlk5/+YPn26u+/E52QJu3f+9xxx7ty5bsmnfv36bh6KyO677z7XYHG0+cYRNX3h999/T3rOZN5vlqG8GNh8MmjQoLgPpnMqddldzjrrLCeKZHVasmSJsyqijkS83oi5W7dubm7MBhbMd4haCfgRoh/ujXr7eTvnWbRoUYZRmSU5YBXBl1u/fn2GVYxk5aLe+MzK0VExVfnmm28y3C9+gNy5c5sWO5i+e7zEmHwvv/xy7BgNwjcg1oQZHWDevHmxMjRS5rnJoNFFO4dJkya5Bl29enX3/UUXXZRBMIxQqdQl+tWP0FnBLjfAyRcFQTDfxpqIjvDRjqJVq1ZudWDgwIHuGHWkLqyxRwWK9cBUxK9KAJ0Gz4AlMg8dAltjsRj8CgJLiFgeQ4cOjZXDksC6YHRnaRMuu+wy15kMGzYsrp58z5TliiuuiD1bOmJWRqJ8/fXXbvXj8oTlOouYNumZ59HoaDiMAKw5YyIOHz48OP74492+bRox2zVZ3mratKlr4DTkPHnyOKdVIozGeK4ZpZctW+bOzTrzbbfdttd1Af7vN60wQjdo0CDTc+Jgq1Wrlls1oF58v3r1andO6p/VqgH1xarAi4+Azz33XDfys2TIBhpWE+gMcQzS+bFcd/HFF7vfpSPhw5yZPf/cP0uAXJuVgagfgSkCgvf3jNMSS6dPnz6xzoi60gEx3aBjoQPgmnSm+A/8M6FDoh7M2bEuqAN/A5Yh8W907NgxmxZhgNA4mzdvDvv06RPWqFEjLFy4cFimTJmwTZs24eLFi2NlJk+eHNatW9dt22QLav/+/cNx48a57ZrLly+P2/I6fvz4sEOHDm4raOnSpcPOnTvHtn7mRF2ArajFixd311i6dGk4ZswYd+1p06ZlOCdbVV966aWwZs2arv5sWaVOK1asyHZr7ezZs8N8+fKFTZo0iR2bP39+2KpVK3dvbGutU6dOOGTIEHedKBs3bnT1LF++vKsn22V5Rols37497NevX1ilShV3vnr16oWTJk1K+nyGDh0aVq9ePSxUqJC7n2Tbi6kHfx/Kcb8VKlQIu3TpEq5evTrT526JXPzzb3c6Qoh/BtNzeCGsIcELYQgJXghDSPBCGEKCF8IQErwQhjApePKrDR48OLjmmmuC4sWLuw0f7GNn33riZhoirkguQWz4P52sMgrbY4k7Z+so+d+IR//2228zlCO4hfol+/hEENnBvn02DvndfOxQI4cccf3J4Nlwfp5VTiTl9Ak9iCfYF6SlpcXtArSEuZ127A0nuQOhnmSBoWGzXRUxsMuN+HFEQ8BGMtgd9sQTT7gQ038KOiF2kLEv/pZbbnEx5OweI5qOhuv3msOPP/7ofv7QQw9lOE9m95R4LTo+7jG675ztGmTRYetwsu24e9OR0XlxX9wjEOjC9elk9lWCzbvvvtttyU2MMDzQMSf49957z+0jp1Ej9ihsIaXx9e3b1zW4ZNBAEhNP7GvYX89ee7an+o4Gi4TwT7a+kqgiKnj2lO9pHRmlCYYhjj8RMt6QyINtrjkFIbwE8kTh+sky8eQU5cqVcxYEAUqPPfZYYAlzJr2P82ZfeSKYy+wHz8x0/bfAtKa+UauCfezsUU9MzIgosUL2BPbuM4In6ywQCCIkEMcnodyfSUtLc4FAa9euDSxhTvA+nhqTONmu4jFjxsQyxSQjszk832MxIEpGR8zfRH8Ac1LMcHwGzMPvuOOOWIhnVpBIk5E1CsEmRK1FzXQCZAj28YIngCSzuP1kEM3G7/jIvihEpTFNYFrhE2RmB0E7RAByrySgJHoPy8onBM0uKSfPi9h4klMSqpvIm2++6cpNmzYtdoyAH3ILEJVHDrs2bdq4AJpEqlWr5jpSa3N5c4Jn9CLWG9MVE56ILhoWAgIi0HYX8sXRYInaIiIL4dPwmB74UFnywJHbjnBQctbxf0ZK5q27Y1EQmkunw+9TZ5JaRs15H8pL5B0dCx/mrMki+xLh5RbMm0mCmQyiz4ja4/p0jFnB80WkpJoipp/nwv9J5uEzz/iknIBIk02jCMslqw6ZeBLfHkPEIPnqsMyAqQ3X4XeIBLzppptcgg1G86VLl2bo+PF9RDsLE4QGmTFjRlipUiUXYeY/RFY1bdo0Q0QXkVz8fNGiRUlfBLFw4UIXUda+ffu4lySMHj3alZs4cWK4YcOGsESJEmGLFi3izs1xotSiL2fIjsaNG8fqTDTatm3bYj/jxQ8c55zDhw8Pp0yZEvbs2TO85JJLwlq1armXQWQGL30gCq1Tp04ZfhaNpCPyjyg+Pj4KkGfDdXlWQJ1KliwZNm/ePENkXOXKlV00YFYv1kiM/iMCke+nTp0aK5Oenu7+Zj169HDfL1u2zN1n9KUWsGrVKhdZ2Lp16wz31bVr17BAgQJZPpcDDXMjPDAikEYKhxe9PwkfGD2IscbEZMkuVRgVmRrwe9E0UrwOChMZ857MNaRnYtT1yR/5MPrzc8x6nHKpgDebd6URR05cOw40PzXBbGbEZJWBeHW80Ly+iYytLJ1lZb5yferDs8gKMuQwijLfz8zhhRcfxyi5+6MwXybbrbemUoUpBnntGNE9kydPdn8z0nAB0zCmL9xz9BljsZGiGitux9/Zezwk4uBYTr+n77+MOS+9h4ZA4/DZUjCvmXOScoqGSuKIZDnTE/HpoxJfpsD5fbYab05mlXCCBJCpXA/PvBcBmWzx4NOY6TjIEOOzxEThDTCIc9asWS6JRzJ8tppUsuOSAIP5M04vpi/JXvDA/dOBYorjhWce7acVu7sUhti5XzppOiXOzfIpCUMTnzEdYmakp6fH5b/z/hyO76mjc3/DlOBJs0T2mrx587o1+MSMM4xcpFJi/klaJC+urPBOsazSKPsNLDi7EjuGrLLdZgf3gOCxEJIto0VHXEZW7j+VlNCpwBtxGF3xXfTr1y/uZ5yDdW7EjhMTUTJHZ5MTaaP3xMvPtehkmHPjkCNlFfnwE58xdcms08qTJ0+GeoKlPHemBI+zDqdRMsF7fJqmzF58kIhP7sgIE90Ag7lJ+iXMeF+GBkcOuyhfffWVM/czux6Nkh2BjKKJL43wprH/XSwIHHYII9qIGb0ZxUgNlRk+d1+qKbHpuJg+ILDEHXaIEbFjTSRaNZkl/8wOHHfUEVMeE5znwrTJ458xIzgdSxT/mupDExyy/l79vVvATtf2dwJIvMxkP2VZLtlojVnP64vwIKeCF1HiMh0Nk5xrjDz4DBAlCSGjCS5pcIxS5KH3ySmTjbw0SEY274UHGjznQ9h+dMdUpgzvmIviBRkVSCJ4uxHE7sxnWVZkkw8ve0wmJH6W6O9gdI/OpVNNykk56s/0hfvjjT5Rf4NPUIkFFz0XS5e8iIN8eLkSrDDuFesnuzTXBxKmRnhgxEHwmJY0HMTCchpOKxIo0iDZaZfsVUnJYB0YBxmZVn2iSBoS39MoWfoj3ztLY+SkZ7T223lxorHOzI4vn7c+GTjeSFbJq5NYasJkpe5sIiKpo59/smWY46zbs8+ejTLMoxEkzslE6yIKDZ9ODosjVeggeI7UKToVYN2dKQTLb9wfnQlTJJYmsbKYWvhc+buTlJMlVSw0ljGZUiRaZiTexAfD3wMfDHsKcGzSkfNMEuFeqWuq1twBQWiQLVu2hAMGDHCJFVleYnmHZbp27dq5JI27sywHLMcNHjzYLX2xtMUyVq9evdyyWxSSYXLNokWLhqVKlXJLbNGlpqxYsGCBW+ZiiYlrNGzYMJwwYUKGcitXrnRJNMuWLevuq3bt2q5uO3fuzPYalOPe1qxZs1vvju/WrVvcshzMmTPH3R/1JeElS488s2HDhrmyJMjck6ScJM3k/tetW5e0LqNGjQrr16/vkoDyDHhX/Ny5czOUS09Pd++mHzhwYGgJJbEUMZjnYxqzOy27tNr7OyNGjHC7/njnHxaIFUzN4UWQ7Ro7U46sXrF1oDB27Fg3fbAkdpDgRRz4AXhhBGveByrTp093vpqcjPrbX5DgRRx4rNn7juMylddZ7Y/07dvXxTOkstHpQENzeCEMoRFeCENI8EIYQoIXwhAp77TLKjhECPHvk0rgk0Z4IQwhwQthCAleCENI8EIYQoIXwhASvBCGkOCFMIQEL4QhJHghDCHBC2EICV4IQ0jwQhhCghfCEBK8EIaQ4IUwhAQvhCEkeCEMIcELYQgJXghDSPBCGEKCF8IQErwQhpDghTCEBC+EISR4IQwhwQthCAleCENI8EIYQoIXwhASvBCGkOCFMIQEL4QhJHghDCHBC2EICV4IQ0jwQhhCghfCEBK8EIaQ4IUwhAQvhCEkeCEMIcELYQgJXghDSPBCGEKCF8IQErwQhpDghTCEBC+EISR4IQwhwQthCAleCENI8EIYQoIXwhASvBCGkOCFMIQEL4QhJHghDCHBC2EICV4IQ0jwQhhCghfCEBK8EIaQ4IUwhAQvhCEkeCEMIcELYQgJXghDSPBCGEKCF8IQErwQhpDghTCEBC+EISR4IQwhwQthCAleCENI8EIYQoIXwhASvBCGkOCFMIQEL4QhJHghDCHBC2EICV4IQ0jwQhhCghfCEBK8EIaQ4IUwhAQvhCEkeCEMIcELYQgJXghDSPBCGEKCF8IQErwQhpDghTCEBC+EISR4IQwhwQthCAleCENI8EIYQoIXwhASvBCGkOCFMIQEL4QhJHghDCHBC2EICV4IQ0jwQhhCghfCEBK8EIaQ4IUwhAQvhCEkeCEMIcELYQgJXghDSPBCGEKCF8IQErwQhpDghTCEBC+EISR4IQwhwQthCAleCENI8EIYQoIXwhASvBCGkOCFMIQEL4QhJHghDCHBC2EICV4IQ0jwQhhCghfCEBK8EIaQ4IUwhAQvhCEkeCEMIcELYQgJXghDSPBCGEKCF8IQErwQhpDghTCEBC+EISR4IQwhwQthCAleCENI8EIYQoIXwhASvBCGkOCFMIQEL4QhJHghDCHBC2EICV4IQ0jwQhhCghfCEBK8EIaQ4IUwhAQvhCEkeCEMIcELYQgJXghDSPBCGEKCF8IQErwQhpDghTCEBC+EISR4IQwhwQthCAleCENI8EIYQoIXwhASvBCGkOCFMIQEL4QhJHghDCHBC2EICV4IQ0jwQhhCghfCEBK8EIaQ4IUwhAQvhCEkeCEMIcELYQgJXghDHJJqwTAM921NhBD7HI3wQhhCghfCEBK8EIaQ4IUwhAQvhCEkeCEMIcELYQgJXghDSPBCBHb4H1qPU+k9dAz8AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. Load a tiny subset from Hugging Face (Stream mode to save RAM)\n",
    "print(\" Loading ISLES'24 stream...\")\n",
    "ds = load_dataset(\"hugging-science/isles24-stroke\", split=\"train\", streaming=True)\n",
    "\n",
    "def inspect_isles_data(dataset, num_samples=3):\n",
    "    print(f\"\\n INSPECTING FIRST {num_samples} SAMPLES\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Iterate through the stream\n",
    "    iterator = iter(dataset)\n",
    "    for i in range(num_samples):\n",
    "        sample = next(iterator)\n",
    "        case_id = sample.get(\"subject_id\", f\"Case {i}\")\n",
    "        \n",
    "        # Extract Mask\n",
    "        mask_obj = sample.get(\"lesion_mask\")\n",
    "        \n",
    "        # Handle HF Image object vs Raw Array\n",
    "        if hasattr(mask_obj, \"get_fdata\"): \n",
    "            # It's a Nifti object\n",
    "            mask = mask_obj.get_fdata()\n",
    "            affine = mask_obj.affine # Important: Contains physical spacing info\n",
    "        elif hasattr(mask_obj, \"numpy\"): \n",
    "            # It's a PIL/Tensor object\n",
    "            mask = mask_obj.numpy()\n",
    "            affine = \"Unknown (Raw Array)\"\n",
    "        else:\n",
    "            mask = np.array(mask_obj)\n",
    "            affine = \"Unknown\"\n",
    "\n",
    "        # Handle 4D (time/channel) vs 3D\n",
    "        if mask.ndim == 4: mask = mask[..., 0]\n",
    "\n",
    "        # 2. Extract Statistics\n",
    "        shape = mask.shape\n",
    "        unique_vals = np.unique(mask)\n",
    "        \n",
    "        print(f\" {case_id}\")\n",
    "        print(f\"    Shape: {shape}\")\n",
    "        print(f\"    Values: {unique_vals} (Should be 0. and 1. for masks)\")\n",
    "        print(f\"    Affine/Spacing Info: {affine}\")\n",
    "\n",
    "        # 3. Visualization Check (Middle Slice)\n",
    "        # We plot the middle slice of the Z-axis to check for distortion\n",
    "        mid_slice_idx = shape[2] // 2\n",
    "        mid_slice = mask[:, :, mid_slice_idx]\n",
    "\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.imshow(mid_slice, cmap=\"gray\")\n",
    "        plt.title(f\"{case_id}\\nSlice {mid_slice_idx} (Native)\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# Run inspection\n",
    "inspect_isles_data(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc54cda-864b-400d-83f6-68bb5469bebc",
   "metadata": {},
   "source": [
    "## PARTE 1: CARGA Y EXPLORACIÓN DE DATOS\n",
    "\n",
    "ISLES'24 contiene imágenes médicas de **149 pacientes con stroke (derrame cerebral)**:\n",
    "\n",
    "| Tipo de Dato | Descripción | Formato |\n",
    "|--------------|-------------|---------|\n",
    "| **Máscaras de lesión** | Imagen 3D binaria que muestra dónde está la lesión | NIfTI (64×64×64 voxels) |\n",
    "| **CT/MRI** | Imágenes cerebrales completas | NIfTI |\n",
    "| **Datos clínicos** | Edad, sexo, severidad (NIHSS), outcome (mRS) | Numéricos |\n",
    "\n",
    "### Nosotros usamos:\n",
    "- **Input**: Máscaras de lesión 3D → comprimidas a vector de 50 números\n",
    "- **Target**: mRS a 3 meses (0-6, donde 0=sin síntomas, 6=muerte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cdeb972c-fec0-4e8f-957f-1b1fd7820144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MONAI Spacing Transform initialized: (2.0, 2.0, 2.0)mm isotropic\n"
     ]
    }
   ],
   "source": [
    "# Initialize MONAI spacing transform (fixes physics/anisotropy)\n",
    "# Resamples all volumes to 2mm isotropic voxels (standard MNI resolution)\n",
    "spacing_transform = Spacing(pixdim=cfg.TARGET_SPACING, mode=\"nearest\")\n",
    "\n",
    "print(f\" MONAI Spacing Transform initialized: {cfg.TARGET_SPACING}mm isotropic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a975dbe-284a-46ea-8d8c-be768552de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_with_pad(image, new_shape):\n",
    "    \"\"\"\n",
    "    Resizes image to fit inside new_shape while maintaining aspect ratio.\n",
    "    Pads with zeros to fill the remaining space.\n",
    "    \n",
    "    CRITICAL: This should only be called AFTER MONAI spacing correction.\n",
    "    \"\"\"\n",
    "    original_shape = np.array(image.shape)\n",
    "    target_shape = np.array(new_shape)\n",
    "    \n",
    "    # Calculate scale needed to fit the longest side\n",
    "    ratios = target_shape / original_shape\n",
    "    scale = np.min(ratios)\n",
    "    new_real_shape = (original_shape * scale).astype(int)\n",
    "    factors = new_real_shape / original_shape\n",
    "    \n",
    "    # Order=0 for binary masks (nearest neighbor)\n",
    "    resized_img = zoom(image, factors, order=0)\n",
    "    \n",
    "    # Pad to reach target_shape\n",
    "    delta = target_shape - new_real_shape\n",
    "    pad_width = [(d // 2, d - d // 2) for d in delta]\n",
    "    padded_img = np.pad(resized_img, pad_width, mode='constant', constant_values=0)\n",
    "    \n",
    "    return padded_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7082590-7e52-4c05-88fe-95a7dfdb1b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_isles_from_hf(n_cases=50, target_shape=(96, 112, 96)):\n",
    "    \"\"\"\n",
    "    Load ISLES data with MONAI spacing-aware preprocessing.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Load raw data + affine from HuggingFace\n",
    "    2. MONAI Spacing: resample to isotropic voxels (physics correction)\n",
    "    3. Aspect-preserving resize + padding to target shape\n",
    "    \n",
    "    Returns:\n",
    "        masks: (N, 1, H, W, D) tensor ready for VAE\n",
    "        clinical: dict with patient metadata\n",
    "    \"\"\"\n",
    "    ds = load_dataset(cfg.HF_DATASET, split=\"train\", streaming=True)\n",
    "    masks = []\n",
    "    clinical = {\"subject_id\": [], \"age\": [], \"sex\": [], \"nihss_admission\": [], \"mrs_3month\": []}\n",
    "    \n",
    "    print(f\" Processing first {n_cases} cases with MONAI spacing...\")\n",
    "    iterator = iter(ds)\n",
    "    \n",
    "    for i in range(n_cases):\n",
    "        try:\n",
    "            ex = next(iterator)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        \n",
    "        mask_obj = ex.get(\"lesion_mask\")\n",
    "        if mask_obj is None:\n",
    "            continue\n",
    "        \n",
    "        # ═══════════════════════════════════════════════════════════\n",
    "        # STEP 1: Load Data & Affine (spacing metadata)\n",
    "        # ═══════════════════════════════════════════════════════════\n",
    "        if hasattr(mask_obj, \"get_fdata\"):\n",
    "            raw_data = mask_obj.get_fdata()\n",
    "            raw_affine = mask_obj.affine\n",
    "        else:\n",
    "            raw_data = np.array(mask_obj)\n",
    "            raw_affine = np.eye(4)  # identity if no affine available\n",
    "        \n",
    "        if raw_data.ndim == 4:\n",
    "            raw_data = raw_data[..., 0]\n",
    "        \n",
    "        # ═══════════════════════════════════════════════════════════\n",
    "        # STEP 2: MONAI Spacing (Physics Correction)\n",
    "        # CRITICAL FIX: This prevents \"pancake brain\" distortion\n",
    "        # ═══════════════════════════════════════════════════════════\n",
    "        # Add channel dim: (W, H, D) -> (1, W, H, D)\n",
    "        raw_tensor = MetaTensor(np.expand_dims(raw_data, 0), affine=raw_affine)\n",
    "        resampled_tensor = spacing_transform(raw_tensor)\n",
    "        \n",
    "        # Remove channel dim: (1, W', H', D') -> (W', H', D')\n",
    "        isotropic_data = resampled_tensor[0].numpy()\n",
    "        \n",
    "        # ═══════════════════════════════════════════════════════════\n",
    "        # STEP 3: Aspect-Preserving Resize + Padding\n",
    "        # ═══════════════════════════════════════════════════════════\n",
    "        final_mask = resize_with_pad(isotropic_data, target_shape)\n",
    "        final_mask = (final_mask > 0.5).astype(np.float32)\n",
    "        \n",
    "        masks.append(final_mask)\n",
    "        \n",
    "        # Collect clinical metadata\n",
    "        clinical[\"subject_id\"].append(ex.get(\"subject_id\", f\"sub-{i}\"))\n",
    "        clinical[\"age\"].append(ex.get(\"age\"))\n",
    "        clinical[\"sex\"].append(ex.get(\"sex\"))\n",
    "        clinical[\"nihss_admission\"].append(ex.get(\"nihss_admission\"))\n",
    "        clinical[\"mrs_3month\"].append(ex.get(\"mrs_3month\"))\n",
    "        \n",
    "        print(f\"   Case {i+1}: {raw_data.shape} -> {isotropic_data.shape} -> {final_mask.shape}\")\n",
    "    \n",
    "    masks = np.array(masks)\n",
    "    \n",
    "    # Add channel dim back for VAE training input: (N, 1, 96, 112, 96)\n",
    "    masks = np.expand_dims(masks, axis=1)\n",
    "    \n",
    "    print(f\"\\n Final Batch Shape: {masks.shape}\")\n",
    "    print(f\" Voxel Spacing: {cfg.TARGET_SPACING}mm (isotropic)\")\n",
    "    print(f\" Target Shape: {cfg.TARGET_SHAPE}\")\n",
    "    \n",
    "    return masks, clinical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83c0363-848e-494f-86a5-ddf198c35583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799768919b234e2fb354ba9644ebff76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing first 149 cases with MONAI spacing...\n",
      "   Case 1: (512, 595, 75) -> (121, 140, 75) -> (96, 112, 96)\n",
      "   Case 2: (512, 605, 75) -> (139, 164, 75) -> (96, 112, 96)\n",
      "   Case 3: (512, 633, 70) -> (106, 131, 70) -> (96, 112, 96)\n",
      "   Case 4: (512, 587, 69) -> (118, 135, 69) -> (96, 112, 96)\n",
      "   Case 5: (512, 536, 69) -> (106, 111, 69) -> (96, 112, 96)\n",
      "   Case 6: (512, 544, 64) -> (106, 112, 64) -> (96, 112, 96)\n",
      "   Case 7: (512, 585, 69) -> (106, 121, 69) -> (96, 112, 96)\n",
      "   Case 8: (512, 554, 80) -> (106, 114, 80) -> (96, 112, 96)\n",
      "   Case 9: (512, 540, 69) -> (122, 128, 69) -> (96, 112, 96)\n",
      "   Case 10: (512, 603, 79) -> (106, 124, 79) -> (96, 112, 96)\n",
      "   Case 11: (512, 631, 74) -> (106, 130, 74) -> (96, 112, 96)\n",
      "   Case 12: (512, 612, 75) -> (106, 126, 75) -> (96, 112, 96)\n",
      "   Case 13: (512, 597, 74) -> (106, 123, 74) -> (96, 112, 96)\n",
      "   Case 14: (512, 586, 74) -> (99, 113, 74) -> (96, 112, 96)\n",
      "   Case 15: (512, 512, 72) -> (101, 101, 72) -> (96, 112, 96)\n",
      "   Case 16: (512, 610, 80) -> (145, 172, 80) -> (96, 112, 96)\n",
      "   Case 17: (512, 602, 65) -> (121, 142, 65) -> (96, 112, 96)\n",
      "   Case 18: (512, 576, 80) -> (106, 119, 80) -> (96, 112, 96)\n",
      "   Case 19: (512, 512, 80) -> (101, 101, 80) -> (96, 112, 96)\n",
      "   Case 20: (512, 531, 74) -> (121, 125, 74) -> (96, 112, 96)\n",
      "   Case 21: (512, 571, 70) -> (126, 140, 70) -> (96, 112, 96)\n",
      "   Case 22: (512, 610, 69) -> (106, 126, 69) -> (96, 112, 96)\n",
      "   Case 23: (512, 605, 80) -> (113, 134, 80) -> (96, 112, 96)\n",
      "   Case 24: (512, 612, 65) -> (106, 126, 65) -> (96, 112, 96)\n",
      "   Case 25: (512, 616, 80) -> (126, 151, 80) -> (96, 112, 96)\n",
      "   Case 26: (512, 586, 64) -> (118, 135, 64) -> (96, 112, 96)\n",
      "   Case 27: (512, 603, 79) -> (106, 124, 79) -> (96, 112, 96)\n",
      "   Case 28: (512, 544, 69) -> (106, 112, 69) -> (96, 112, 96)\n",
      "   Case 29: (512, 550, 48) -> (135, 145, 71) -> (96, 112, 96)\n",
      "   Case 30: (512, 558, 65) -> (126, 137, 65) -> (96, 112, 96)\n",
      "   Case 31: (512, 512, 177) -> (126, 126, 71) -> (96, 112, 96)\n",
      "   Case 32: (512, 615, 74) -> (115, 138, 74) -> (96, 112, 96)\n",
      "   Case 33: (512, 655, 74) -> (106, 135, 74) -> (96, 112, 96)\n",
      "   Case 34: (512, 595, 69) -> (106, 123, 69) -> (96, 112, 96)\n",
      "   Case 35: (512, 616, 70) -> (106, 127, 70) -> (96, 112, 96)\n",
      "   Case 36: (512, 572, 79) -> (106, 118, 79) -> (96, 112, 96)\n",
      "   Case 37: (512, 556, 75) -> (106, 115, 75) -> (96, 112, 96)\n",
      "   Case 38: (512, 584, 64) -> (128, 146, 64) -> (96, 112, 96)\n",
      "   Case 39: (512, 553, 70) -> (106, 114, 70) -> (96, 112, 96)\n",
      "   Case 40: (512, 553, 70) -> (106, 114, 70) -> (96, 112, 96)\n",
      "   Case 41: (512, 569, 65) -> (131, 146, 65) -> (96, 112, 96)\n",
      "   Case 42: (512, 562, 69) -> (106, 116, 69) -> (96, 112, 96)\n",
      "   Case 43: (512, 590, 68) -> (118, 136, 68) -> (96, 112, 96)\n",
      "   Case 44: (512, 559, 70) -> (106, 115, 70) -> (96, 112, 96)\n",
      "   Case 45: (512, 561, 70) -> (126, 138, 70) -> (96, 112, 96)\n",
      "   Case 46: (512, 599, 65) -> (121, 141, 65) -> (96, 112, 96)\n",
      "   Case 47: (512, 558, 65) -> (126, 137, 65) -> (96, 112, 96)\n",
      "   Case 48: (512, 527, 70) -> (126, 129, 70) -> (96, 112, 96)\n",
      "   Case 49: (512, 581, 74) -> (106, 120, 74) -> (96, 112, 96)\n",
      "   Case 50: (512, 643, 75) -> (119, 149, 75) -> (96, 112, 96)\n",
      "   Case 51: (512, 512, 71) -> (110, 110, 71) -> (96, 112, 96)\n",
      "   Case 52: (512, 512, 74) -> (110, 110, 74) -> (96, 112, 96)\n",
      "   Case 53: (512, 512, 74) -> (107, 107, 74) -> (96, 112, 96)\n",
      "   Case 54: (512, 551, 74) -> (106, 114, 74) -> (96, 112, 96)\n",
      "   Case 55: (512, 606, 69) -> (106, 125, 69) -> (96, 112, 96)\n",
      "   Case 56: (512, 588, 70) -> (106, 121, 70) -> (96, 112, 96)\n",
      "   Case 57: (512, 539, 69) -> (106, 111, 69) -> (96, 112, 96)\n",
      "   Case 58: (512, 512, 73) -> (118, 118, 73) -> (96, 112, 96)\n",
      "   Case 59: (512, 512, 75) -> (109, 109, 75) -> (96, 112, 96)\n",
      "   Case 60: (512, 533, 70) -> (106, 110, 70) -> (96, 112, 96)\n",
      "   Case 61: (512, 578, 69) -> (106, 119, 69) -> (96, 112, 96)\n",
      "   Case 62: (512, 512, 69) -> (99, 99, 69) -> (96, 112, 96)\n",
      "   Case 63: (512, 512, 70) -> (96, 96, 70) -> (96, 112, 96)\n",
      "   Case 64: (512, 533, 69) -> (106, 110, 69) -> (96, 112, 96)\n",
      "   Case 65: (512, 638, 79) -> (106, 132, 79) -> (96, 112, 96)\n",
      "   Case 66: (512, 594, 69) -> (106, 123, 69) -> (96, 112, 96)\n",
      "   Case 67: (512, 590, 69) -> (126, 145, 69) -> (96, 112, 96)\n",
      "   Case 68: (512, 652, 89) -> (117, 148, 89) -> (96, 112, 96)\n",
      "   Case 69: (512, 512, 71) -> (104, 104, 71) -> (96, 112, 96)\n",
      "   Case 70: (512, 577, 74) -> (98, 111, 74) -> (96, 112, 96)\n",
      "   Case 71: (512, 563, 69) -> (106, 116, 69) -> (96, 112, 96)\n",
      "   Case 72: (512, 619, 74) -> (106, 128, 74) -> (96, 112, 96)\n",
      "   Case 73: (512, 512, 69) -> (107, 107, 69) -> (96, 112, 96)\n",
      "   Case 74: (512, 512, 71) -> (105, 105, 71) -> (96, 112, 96)\n",
      "   Case 75: (512, 620, 64) -> (121, 146, 64) -> (96, 112, 96)\n",
      "   Case 76: (512, 512, 68) -> (106, 106, 68) -> (96, 112, 96)\n",
      "   Case 77: (512, 561, 75) -> (106, 116, 75) -> (96, 112, 96)\n",
      "   Case 78: (512, 512, 69) -> (97, 97, 69) -> (96, 112, 96)\n",
      "   Case 79: (512, 512, 75) -> (110, 110, 75) -> (96, 112, 96)\n",
      "   Case 80: (512, 512, 69) -> (106, 106, 69) -> (96, 112, 96)\n",
      "   Case 81: (512, 587, 75) -> (106, 121, 75) -> (96, 112, 96)\n",
      "   Case 82: (512, 512, 69) -> (109, 109, 69) -> (96, 112, 96)\n",
      "   Case 83: (512, 512, 72) -> (112, 112, 72) -> (96, 112, 96)\n",
      "   Case 84: (512, 636, 69) -> (106, 131, 69) -> (96, 112, 96)\n",
      "   Case 85: (512, 512, 69) -> (106, 106, 69) -> (96, 112, 96)\n",
      "   Case 86: (512, 512, 76) -> (103, 103, 76) -> (96, 112, 96)\n",
      "   Case 87: (512, 613, 74) -> (106, 127, 74) -> (96, 112, 96)\n",
      "   Case 88: (512, 512, 65) -> (100, 100, 65) -> (96, 112, 96)\n",
      "   Case 89: (512, 593, 64) -> (106, 122, 64) -> (96, 112, 96)\n",
      "   Case 90: (512, 544, 74) -> (106, 112, 74) -> (96, 112, 96)\n",
      "   Case 91: (512, 512, 69) -> (95, 95, 69) -> (96, 112, 96)\n",
      "   Case 92: (512, 512, 74) -> (115, 115, 74) -> (96, 112, 96)\n",
      "   Case 93: (512, 518, 69) -> (117, 119, 69) -> (96, 112, 96)\n",
      "   Case 94: (512, 686, 45) -> (88, 117, 89) -> (96, 112, 96)\n",
      "   Case 95: (512, 644, 42) -> (88, 110, 83) -> (96, 112, 96)\n",
      "   Case 96: (512, 658, 40) -> (92, 118, 79) -> (96, 112, 96)\n",
      "   Case 97: (512, 665, 44) -> (80, 104, 87) -> (96, 112, 96)\n",
      "   Case 98: (512, 512, 85) -> (109, 109, 85) -> (96, 112, 96)\n",
      "   Case 99: (512, 617, 41) -> (90, 108, 81) -> (96, 112, 96)\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos\n",
    "masks, clinical_data = load_isles_from_hf(n_cases=cfg.N_CASES, target_shape=cfg.TARGET_SHAPE)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" LOADED DATA SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Masks shape: {masks.shape}\")\n",
    "print(f\"Data type: {masks.dtype}\")\n",
    "print(f\"Value range: [{masks.min():.3f}, {masks.max():.3f}]\")\n",
    "print(f\"Patients with mRS outcomes: {sum(x is not None for x in clinical_data['mrs_3month'])}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddc7a23-0409-4c5f-8b7c-ae50c6ac66c4",
   "metadata": {},
   "source": [
    "### Visualización de datos\n",
    "\n",
    "Las máscaras son **imágenes 3D binarias** donde:\n",
    "- **Blanco (1)** = Tejido dañado por el stroke\n",
    "- **Negro (0)** = Tejido sano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b503658-9fcf-4a7e-b1bc-97588f3c8d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Visualización de 3 casos (cortes + MIP)\n",
    "def visualize_lesion_mask(mask, subject_id=\"\", slice_idx=None):\n",
    "    \"\"\"Visualize 3D lesion mask with orthogonal slices + MIP\"\"\"\n",
    "    # Remove channel dimension if present\n",
    "    if mask.ndim == 4:\n",
    "        mask = mask[0]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    if slice_idx is None:\n",
    "        slice_idx = [s // 2 for s in mask.shape]\n",
    "    \n",
    "    axes[0].imshow(mask[slice_idx[0], :, :], cmap='hot', vmin=0, vmax=1)\n",
    "    axes[0].set_title(f'Axial z={slice_idx[0]}')\n",
    "    \n",
    "    axes[1].imshow(mask[:, slice_idx[1], :], cmap='hot', vmin=0, vmax=1)\n",
    "    axes[1].set_title(f'Coronal y={slice_idx[1]}')\n",
    "    \n",
    "    axes[2].imshow(mask[:, :, slice_idx[2]], cmap='hot', vmin=0, vmax=1)\n",
    "    axes[2].set_title(f'Sagital x={slice_idx[2]}')\n",
    "    \n",
    "    mip = mask.max(axis=0)\n",
    "    axes[3].imshow(mip, cmap='hot', vmin=0, vmax=1)\n",
    "    axes[3].set_title('MIP (Maximum Intensity Projection)')\n",
    "    \n",
    "    lesion_volume = mask.sum()\n",
    "    total_volume = mask.size\n",
    "    perc = 100 * lesion_volume / total_volume\n",
    "    \n",
    "    fig.suptitle(f'Máscara {subject_id} | Voxels: {lesion_volume:.0f} ({perc:.2f}%)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return lesion_volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee444a9-4685-4b60-ad3c-b2b9497a492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar primeros 3 casos\n",
    "print(\" VISUALIZACIÓN DE MÁSCARAS DE LESIÓN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(min(3, len(masks))):\n",
    "    sid = clinical_data['subject_id'][i]\n",
    "    mrs = clinical_data['mrs_3month'][i]\n",
    "    print(f\"\\nPaciente {sid} | Outcome (mRS): {mrs}\")\n",
    "    visualize_lesion_mask(masks[i], sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d8a616-8456-4fc8-bf3a-405bd0f5dcf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Crear DataFrame con datos clínicos\n",
    "df_clinical = pd.DataFrame(clinical_data)\n",
    "\n",
    "# Calcular volumen de lesión para cada paciente\n",
    "df_clinical['lesion_volume'] = [m.sum() for m in masks]\n",
    "df_clinical['lesion_percentage'] = df_clinical['lesion_volume'] / (64**3) * 100\n",
    "\n",
    "print(\" DATOS CLÍNICOS DE LOS PACIENTES\")\n",
    "print(\"=\"*60)\n",
    "df_clinical.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3d5c50-d509-4265-b374-db722eb745a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ESTADÍSTICAS DESCRIPTIVAS\")\n",
    "print(\"=\"*60)\n",
    "df_clinical.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc62f30-1350-49be-86a8-9b8f3d22d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de distribuciones\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Distribución de mRS (outcome)\n",
    "mrs_values = [v for v in clinical_data['mrs_3month'] if v is not None]\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.1, 0.9, 7))\n",
    "axes[0, 0].hist(mrs_values, bins=range(8), align='left', color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('mRS a 3 meses')\n",
    "axes[0, 0].set_ylabel('Número de pacientes')\n",
    "axes[0, 0].set_title(' Distribución del Outcome (mRS)\\n0=Sin síntomas, 6=Muerte')\n",
    "axes[0, 0].set_xticks(range(7))\n",
    "\n",
    "# 2. Volumen de lesión vs Outcome\n",
    "valid_idx = [i for i, v in enumerate(clinical_data['mrs_3month']) if v is not None]\n",
    "volumes = [masks[i].sum() for i in valid_idx]\n",
    "mrs_valid = [clinical_data['mrs_3month'][i] for i in valid_idx]\n",
    "scatter = axes[0, 1].scatter(volumes, mrs_valid, c=mrs_valid, cmap='RdYlGn_r', \n",
    "                              s=100, alpha=0.7, edgecolors='black')\n",
    "axes[0, 1].set_xlabel('Volumen de lesión (voxels)')\n",
    "axes[0, 1].set_ylabel('mRS a 3 meses')\n",
    "axes[0, 1].set_title(' Relación: Tamaño de Lesión vs Outcome')\n",
    "plt.colorbar(scatter, ax=axes[0, 1], label='mRS')\n",
    "\n",
    "# 3. Distribución de volúmenes de lesión\n",
    "lesion_volumes = [m.sum() for m in masks]\n",
    "axes[1, 0].hist(lesion_volumes, bins=20, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Volumen de lesión (voxels)')\n",
    "axes[1, 0].set_ylabel('Frecuencia')\n",
    "axes[1, 0].set_title(' Distribución de Tamaños de Lesión')\n",
    "axes[1, 0].axvline(np.mean(lesion_volumes), color='red', linestyle='--', label=f'Media: {np.mean(lesion_volumes):.0f}')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. NIHSS vs Outcome\n",
    "nihss_valid = [clinical_data['nihss_admission'][i] for i in valid_idx if clinical_data['nihss_admission'][i] is not None]\n",
    "mrs_nihss = [clinical_data['mrs_3month'][i] for i in valid_idx if clinical_data['nihss_admission'][i] is not None]\n",
    "if nihss_valid:\n",
    "    axes[1, 1].scatter(nihss_valid, mrs_nihss, c=mrs_nihss, cmap='RdYlGn_r', \n",
    "                       s=100, alpha=0.7, edgecolors='black')\n",
    "    axes[1, 1].set_xlabel('NIHSS al ingreso (severidad)')\n",
    "    axes[1, 1].set_ylabel('mRS a 3 meses')\n",
    "    axes[1, 1].set_title(' NIHSS (severidad inicial) vs Outcome')\n",
    "\n",
    "plt.suptitle('ANÁLISIS EXPLORATORIO DE DATOS CLÍNICOS', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed830904-a42a-4f54-9143-3ffa21951558",
   "metadata": {},
   "source": [
    "# VARIATIONAL AUTOENCODER  3D (VAE)\n",
    "\n",
    "Comprime una imagen 3D de **262,144 voxels** (64×64×64) a un **vector de 50 números**.\n",
    "\n",
    "```\n",
    "Máscara 3D [64, 64, 64] → Encoder → Vector Z [50] → Decoder → Reconstrucción [64, 64, 64]\n",
    "     262,144 valores              50 valores              262,144 valores\n",
    "```\n",
    "\n",
    "El vector Z de 50 números captura la \"esencia\" de la lesión (tamaño, forma, ubicación)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bef85b-7355-4844-81a9-6aaa4b0220c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para entrenamiento: Dataset para VAE\n",
    "class MaskDataset(Dataset):\n",
    "    def __init__(self, masks_np):\n",
    "        self.m = torch.from_numpy(masks_np).float()\n",
    "    def __len__(self): return len(self.m)\n",
    "    def __getitem__(self, idx): return self.m[idx] \n",
    "\n",
    "dataset = MaskDataset(masks)\n",
    "dataloader = DataLoader(dataset, batch_size=cfg.VAE_BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1240ca-fc7c-4d2c-9e85-77c4e3eb26ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del VAE 3D\n",
    "\n",
    "class ResBlock3D(nn.Module):\n",
    "    \"\"\"Residual block para VAE más estable\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(in_ch, out_ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv3d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(out_ch)\n",
    "        self.bn2 = nn.BatchNorm3d(out_ch)\n",
    "        self.skip = nn.Conv3d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = self.skip(x)\n",
    "        out = F.leaky_relu(self.bn1(self.conv1(x)), 0.2)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        return F.leaky_relu(out + identity, 0.2)\n",
    "\n",
    "\n",
    "class AdaptiveEncoder3D(nn.Module):\n",
    "    \"\"\"Encoder que calcula automáticamente las dimensiones\"\"\"\n",
    "    def __init__(self, input_shape, latent_dim=128, base_channels=32):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Bloques convolucionales\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv3d(1, base_channels, 4, 2, 1),\n",
    "                nn.BatchNorm3d(base_channels),\n",
    "                nn.LeakyReLU(0.2)\n",
    "            ),\n",
    "            ResBlock3D(base_channels, base_channels*2),\n",
    "            nn.Sequential(nn.Conv3d(base_channels*2, base_channels*2, 4, 2, 1), nn.LeakyReLU(0.2)),\n",
    "            ResBlock3D(base_channels*2, base_channels*4),\n",
    "            nn.Sequential(nn.Conv3d(base_channels*4, base_channels*4, 4, 2, 1), nn.LeakyReLU(0.2)),\n",
    "            ResBlock3D(base_channels*4, base_channels*8),\n",
    "            nn.Sequential(nn.Conv3d(base_channels*8, base_channels*8, 4, 2, 1), nn.LeakyReLU(0.2)),\n",
    "        ])\n",
    "        \n",
    "        # Calcular tamaño del feature map después de las convoluciones\n",
    "        self.feature_size = self._get_feature_size()\n",
    "        \n",
    "        self.fc_mu = nn.Linear(self.feature_size, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.feature_size, latent_dim)\n",
    "    \n",
    "    def _get_feature_size(self):\n",
    "        \"\"\"Calcula el tamaño del feature map dinámicamente\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, 1, *self.input_shape)\n",
    "            for block in self.blocks:\n",
    "                x = block(x)\n",
    "            return x.numel()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc_mu(x), self.fc_logvar(x)\n",
    "\n",
    "class LesionVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=50):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder3D(latent_dim)\n",
    "        self.decoder = Decoder3D(latent_dim)\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        return mu + torch.randn_like(std)*std\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "    def encode(self, x):\n",
    "        mu, _ = self.encoder(x)\n",
    "        return mu\n",
    "\n",
    "vae = LesionVAE(latent_dim=cfg.LATENT_DIM).to(device)\n",
    "optimizer_vae = optim.Adam(vae.parameters(), lr=cfg.VAE_LR)\n",
    "\n",
    "print(\" VAE definido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152f6b2a-7485-4e8b-8117-c2fd1e3890fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar VAE\n",
    "def vae_loss(recon, original, mu, logvar, beta=1.0):\n",
    "    recon_loss = nn.functional.binary_cross_entropy(recon, original, reduction='sum')\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + beta * kl\n",
    "\n",
    "losses = []\n",
    "for epoch in range(cfg.VAE_EPOCHS):\n",
    "    vae.train(); epoch_loss=0\n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer_vae.zero_grad()\n",
    "        recon, mu, logvar = vae(batch)\n",
    "        loss = vae_loss(recon, batch, mu, logvar)\n",
    "        loss.backward(); optimizer_vae.step()\n",
    "        epoch_loss += loss.item()\n",
    "    avg = epoch_loss / len(dataset)\n",
    "    losses.append(avg)\n",
    "    if (epoch+1)%5==0: print(f\"Epoch {epoch+1}/{cfg.VAE_EPOCHS} loss {avg:,.0f}\")\n",
    "        \n",
    "print(\"-\" * 50)\n",
    "print(\" Entrenamiento completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0958605f-0872-4b4d-ba8b-442c4f2685b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar curva de aprendizaje\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, 'b-', linewidth=2, marker='o', markersize=4)\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(' Curva de Aprendizaje del VAE')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e66190-0d73-4a1e-99c8-d10a2ea2e1f7",
   "metadata": {},
   "source": [
    "## Extraer Vectores Latentes Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b913fcfb-d0b1-45c9-b489-66f0bbe54abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Extraer latentes Z\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    Z_list=[]\n",
    "    for batch in DataLoader(dataset, batch_size=cfg.VAE_BATCH_SIZE):\n",
    "        z = vae.encode(batch.to(device)).cpu()\n",
    "        Z_list.append(z)\n",
    "    Z = torch.cat(Z_list, dim=0).numpy()\n",
    "print(\"Z shape:\", Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed304d55-a99e-450b-ad10-4194871ddd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Visualizar reconstrucciones del VAE\n",
    "def visualize_reconstruction(vae, dataset, num=3):\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(min(num, len(dataset))):\n",
    "            x = dataset[i].unsqueeze(0).to(device)  # [1,1,64,64,64]\n",
    "            recon, _, _ = vae(x)\n",
    "            x_np = x.cpu().numpy()[0,0]\n",
    "            r_np = recon.cpu().numpy()[0,0]\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(12,4))\n",
    "            mid = x_np.shape[0]//2\n",
    "            axes[0].imshow(x_np[mid], cmap=\"hot\"); axes[0].set_title(\"Original (corte axial)\")\n",
    "            axes[1].imshow(r_np[mid], cmap=\"hot\"); axes[1].set_title(\"Reconstrucción\")\n",
    "            diff = np.abs(x_np - r_np)\n",
    "            axes[2].imshow(diff[mid], cmap=\"magma\"); axes[2].set_title(\"Error abs.\")\n",
    "            plt.tight_layout(); plt.show()\n",
    "\n",
    "visualize_reconstruction(vae, dataset, num=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505c55a9-e1ca-4a24-a68a-468044739dad",
   "metadata": {},
   "source": [
    "## InterSynth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b265f5-8b98-4ffd-b3c4-b22d95327449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERSYNTH (Integración Allen Brain Atlas)\n",
    "\n",
    "class AllenWeightMap:\n",
    "    def __init__(self, atlas_path, target_spacing, target_shape, mode=\"bilinear\"):\n",
    "        \"\"\"\n",
    "        atlas_path: ruta al NIfTI de densidad de receptores del Allen Brain Atlas\n",
    "        target_spacing: tu spacing destino, p.ej. (2.0, 2.0, 2.0)\n",
    "        target_shape: tu grid destino, p.ej. (96, 112, 96)\n",
    "        mode: interpolación para re-muestreo ('bilinear' o 'nearest')\n",
    "        \"\"\"\n",
    "        self.atlas_path = atlas_path\n",
    "        self.target_spacing = target_spacing\n",
    "        self.target_shape = target_shape\n",
    "        self.mode = mode\n",
    "        self.spacing_transform = Spacing(pixdim=target_spacing, mode=mode)\n",
    "\n",
    "        self.weight_map = self._load_and_resample()\n",
    "\n",
    "    def _load_and_resample(self):\n",
    "        img = nib.load(self.atlas_path)\n",
    "        data = img.get_fdata()\n",
    "        affine = img.affine\n",
    "\n",
    "        # Añadir dim de canal para MONAI\n",
    "        tensor = MetaTensor(np.expand_dims(data, 0), affine=affine)\n",
    "        resampled = self.spacing_transform(tensor)[0].numpy()  # [H', W', D']\n",
    "\n",
    "        # Resize con aspect-ratio (fit + pad) al target_shape\n",
    "        resized = self._resize_with_pad(resampled, self.target_shape)\n",
    "        return resized.astype(np.float32)\n",
    "\n",
    "    def _resize_with_pad(self, image, new_shape):\n",
    "        orig = np.array(image.shape)\n",
    "        tgt = np.array(new_shape)\n",
    "        ratios = tgt / orig\n",
    "        scale = np.min(ratios)\n",
    "        new_real = (orig * scale).astype(int)\n",
    "        factors = new_real / orig\n",
    "        # bilinear para continuo; nearest si quieres preservar discretos\n",
    "        resized_img = torch.from_numpy(image).unsqueeze(0).unsqueeze(0).float()\n",
    "        resized_img = torch.nn.functional.interpolate(\n",
    "            resized_img,\n",
    "            size=tuple(new_real.tolist()),\n",
    "            mode=\"trilinear\" if self.mode != \"nearest\" else \"nearest\",\n",
    "            align_corners=False if self.mode != \"nearest\" else None,\n",
    "        ).squeeze().numpy()\n",
    "\n",
    "        delta = tgt - new_real\n",
    "        pad_width = [(d // 2, d - d // 2) for d in delta]\n",
    "        padded = np.pad(resized_img, pad_width, mode=\"constant\", constant_values=0)\n",
    "        return padded\n",
    "\n",
    "    def get(self):\n",
    "        return self.weight_map  # np.ndarray [H, W, D], float32\n",
    "\n",
    "\n",
    "def compute_intersynth_score(masks_np, weight_map_np, binarize=True):\n",
    "    \"\"\"\n",
    "    masks_np: np.ndarray [B, H, W, D] (o [B, 1, H, W, D]) binaria o continua\n",
    "    weight_map_np: np.ndarray [H, W, D] continua (densidad de receptores re-muestreada)\n",
    "    binarize: si True, umbral 0.5 sobre la máscara\n",
    "    return: np.ndarray [B] con el score (dot product voxel-wise)\n",
    "    \"\"\"\n",
    "    if masks_np.ndim == 5:\n",
    "        masks_np = masks_np[:, 0]  # quitar canal si viene con [B,1,H,W,D]\n",
    "    if binarize:\n",
    "        masks_np = (masks_np > 0.5).astype(np.float32)\n",
    "\n",
    "    w = torch.from_numpy(weight_map_np).float()               # [H, W, D]\n",
    "    m = torch.from_numpy(masks_np).float()                    # [B, H, W, D]\n",
    "    scores = (m * w).sum(dim=(1, 2, 3))                       # [B]\n",
    "    return scores.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fca97a-f504-4ed7-bc95-c11d1df62337",
   "metadata": {},
   "outputs": [],
   "source": [
    "allen = AllenWeightMap(cfg.ALLEN_ATLAS_PATH, cfg.TARGET_SPACING, cfg.TARGET_SHAPE)\n",
    "weight_map = allen.get()\n",
    "Y_intersynth = compute_intersynth_score(masks, weight_map)\n",
    "print(f\" Y (Outcome) generado. Ejemplo: {Y_intersynth[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a956626-2014-4be8-9b72-a1a4f5ea08fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Conexión híbrida: genera T,Y_scm sobre Z reales\n",
    "Z_real_torch = torch.from_numpy(Z).float().to(device)\n",
    "logits_t = Z_real_torch[:,0]*2.0 + torch.randn_like(Z_real_torch[:,0])\n",
    "prob_t = torch.sigmoid(logits_t)\n",
    "T_real = torch.bernoulli(prob_t).unsqueeze(-1)          # [N,1]\n",
    "Y_real_scm = cfg.SCM_EFFECT * T_real - cfg.SCM_WEIGHT_Z * Z_real_torch[:,0:1]  # [N,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f67f648-7f83-4db0-ad3b-de79fa530ee6",
   "metadata": {},
   "source": [
    "# TabICLTwoStage (ex MiniNeuroTabPFN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2092a9-97f2-4143-abd7-61f971051a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding por columna (cada feature es un token escalar).\n",
    "    x: [B, S, F] -> devuelve [B*F, S, d_model]\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, n_features: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(1, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.n_features = n_features\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [B, S, F]\n",
    "        B, S, F = x.shape\n",
    "        assert F == self.n_features, f\"n_features={self.n_features}, got {F}\"\n",
    "        x = x.permute(0, 2, 1).contiguous()      # [B, F, S]\n",
    "        x = x.view(B * F, S, 1)                  # cada columna es un token\n",
    "        x = self.proj(x)                         # [B*F, S, d_model]\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ColumnBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Atención por columnas (feature-first). Procesa cada columna como una secuencia.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\",\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [B*F, S, d_model]\n",
    "        return self.encoder(x)  # misma forma\n",
    "\n",
    "\n",
    "class RowBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Atención por filas (row-first). Toma las salidas de columnas y las concatena por fila.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, n_features: int, nhead: int, num_layers: int, dim_feedforward: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.d_model = d_model\n",
    "        self.row_dim = d_model * n_features\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.row_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\",\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, h_col: torch.Tensor) -> torch.Tensor:\n",
    "        # h_col: [B, S, F, d_model] -> flatten features -> [B, S, F*d_model]\n",
    "        B, S, F, D = h_col.shape\n",
    "        assert F == self.n_features and D == self.d_model\n",
    "        h = h_col.view(B, S, F * D)\n",
    "        return self.encoder(h)  # [B, S, F*d_model]\n",
    "\n",
    "\n",
    "class TabICLTwoStage(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-Stage Attention (TabICL):\n",
    "    - Etapa 1: Atención por columnas (cada feature es un token).\n",
    "    - Etapa 2: Atención por filas (concatena features por fila y aplica Transformer).\n",
    "    - Cabeza final para regresión (o binaria si cambias el head).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        d_model: int = 128,\n",
    "        nhead_col: int = 4,\n",
    "        nhead_row: int = 4,\n",
    "        num_layers_col: int = 1,\n",
    "        num_layers_row: int = 4,\n",
    "        dim_feedforward: int = 256,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Embedding + columna\n",
    "        self.embed = FeatureEmbed(d_model=d_model, n_features=n_features, dropout=dropout)\n",
    "        self.col_blocks = nn.ModuleList(\n",
    "            [ColumnBlock(d_model, nhead_col, dim_feedforward, dropout) for _ in range(num_layers_col)]\n",
    "        )\n",
    "\n",
    "        # Fila\n",
    "        self.row_block = RowBlock(\n",
    "            d_model=d_model,\n",
    "            n_features=n_features,\n",
    "            nhead=nhead_row,\n",
    "            num_layers=num_layers_row,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        # Cabeza final (regresión)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model * n_features),\n",
    "            nn.Linear(d_model * n_features, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [B, S, F] con features ya concatenados (p.ej. [Z, T, Ymask]).\n",
    "        Devuelve: [B, S, 1]\n",
    "        \"\"\"\n",
    "        B, S, F = x.shape\n",
    "        assert F == self.n_features\n",
    "\n",
    "        # Etapa columnas\n",
    "        h = self.embed(x)  # [B*F, S, d]\n",
    "        for blk in self.col_blocks:\n",
    "            h = blk(h)     # [B*F, S, d]\n",
    "        # reshape a [B, S, F, d]\n",
    "        h = h.view(B, F, S, self.d_model).permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        # Etapa filas\n",
    "        h_row = self.row_block(h)  # [B, S, F*d]\n",
    "\n",
    "        # Pred\n",
    "        out = self.head(h_row)     # [B, S, 1]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab1f981-193d-4296-8990-30c1a3b0ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TabICLTwoStage(\n",
    "    n_features=cfg.LATENT_DIM + 2,  # Z + T + Ymask\n",
    "    d_model=cfg.D_MODEL,\n",
    "    nhead_col=cfg.N_HEAD,\n",
    "    nhead_row=cfg.N_HEAD,\n",
    "    num_layers_col=1,          # subir a 2-3 si tienes VRAM\n",
    "    num_layers_row=cfg.N_LAYERS,\n",
    "    dim_feedforward=256,       # subir a 512/1024 para mayor capacidad\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "x_input = torch.cat([Z_in, T_in, Y_in], dim=-1)\n",
    "pred = model(x_input)\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=cfg.LR_TABICL)\n",
    "loss_fn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaa7d82-6aa2-41cd-98fc-2f004415b71d",
   "metadata": {},
   "source": [
    "# Generador SCM con Confounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd16ab56-a097-4620-87d1-16974a766714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Utils SCM sintético (para training y PEHE)\n",
    "def make_synth_batch(batch_size=cfg.SYN_BATCH, seq_len=cfg.SYN_SEQ, n_features=cfg.LATENT_DIM, on_real_Z=None):\n",
    "    if on_real_Z is None:\n",
    "        z = torch.randn(batch_size, seq_len, n_features, device=device)\n",
    "    else:\n",
    "        N = on_real_Z.shape[0]\n",
    "        idx = torch.randint(0, N, (batch_size*seq_len,), device=device)\n",
    "        z = on_real_Z[idx].view(batch_size, seq_len, n_features)\n",
    "    logits_t = z[:,:,0]*2.0 + torch.randn_like(z[:,:,0])\n",
    "    prob_t = torch.sigmoid(logits_t)\n",
    "    t = torch.bernoulli(prob_t).unsqueeze(-1)\n",
    "    y = cfg.SCM_EFFECT * t - cfg.SCM_WEIGHT_Z * z[:,:,0:1]\n",
    "    return z, t, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ad5da0-a430-4ade-995b-7e022be4e212",
   "metadata": {},
   "source": [
    "# Do-Loss Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ea9ea-447e-49fa-9eed-46ca2648e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_do_step(z, t, y_true, steps=1):\n",
    "    model.train()\n",
    "    for _ in range(steps):\n",
    "        B,S,F = z.shape\n",
    "        Z_ctx, Z_q = z[:,:-1,:], z[:,-1:,:]\n",
    "        T_ctx = t[:,:-1,:]\n",
    "        Y_ctx = y_true[:,:-1,:]\n",
    "        T_q = torch.bernoulli(torch.full_like(t[:,-1:,:], 0.5))\n",
    "        Y_q_true = cfg.SCM_EFFECT * T_q - cfg.SCM_WEIGHT_Z * Z_q[:,:,0:1]\n",
    "        Z_in = torch.cat([Z_ctx, Z_q], dim=1)\n",
    "        T_in = torch.cat([T_ctx, T_q], dim=1)\n",
    "        Y_in = torch.cat([Y_ctx, torch.zeros_like(Y_q_true)], dim=1)\n",
    "        pred = model(Z_in, T_in, Y_in)\n",
    "        pred_q = pred[:,-1:,:]\n",
    "        loss = loss_fn(pred_q, Y_q_true)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16618fe6-d413-483d-ba75-2593e9c7faae",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_causal=[]\n",
    "for step in range(200):\n",
    "    z,t,y = make_synth_batch()\n",
    "    loss = train_do_step(z,t,y,steps=1)\n",
    "    loss_causal.append(loss)\n",
    "    if (step+1)%20==0:\n",
    "        print(f\"Do-step {step+1}: loss {loss:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(loss_causal, label='Causal MSE (query)')\n",
    "plt.title('Curva de entrenamiento causal (Do-Loss)')\n",
    "plt.xlabel('Step'); plt.ylabel('Loss'); plt.grid(True, alpha=0.3); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72476d7b-e192-4539-b43f-52c288d38d45",
   "metadata": {},
   "source": [
    "# Benchmark CATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07985da2-4922-4413-9ab5-3e54e1ec15db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Benchmark CATE sintético (~2.0)\n",
    "@torch.no_grad()\n",
    "def predict_cate(model, z_base, t_base):\n",
    "    z_new = torch.randn_like(z_base[:, :1, :])\n",
    "    def pred_for(tval):\n",
    "        Zin = torch.cat([z_base, z_new], 1)\n",
    "        Tin = torch.cat([t_base, torch.full_like(t_base[:,:1,:], tval)], 1)\n",
    "        Yin = torch.zeros_like(Tin)\n",
    "        out = model(Zin, Tin, Yin)\n",
    "        return out[:,-1,0]\n",
    "    y1 = pred_for(1.0); y0 = pred_for(0.0)\n",
    "    return (y1 - y0).mean().item()\n",
    "\n",
    "z_b,t_b,_ = make_synth_batch(batch_size=4)\n",
    "cate_hat = predict_cate(model, z_b, t_b)\n",
    "print(f\"CATE pred (synth): {cate_hat:.3f} (esperado ~2.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0626a8-fb36-4fe8-8d07-fa67b578fd83",
   "metadata": {},
   "source": [
    "# PEHE Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4304e57b-69f2-4847-8f71-7c0277baadca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% PEHE en sintético\n",
    "@torch.no_grad()\n",
    "def pehe(model, batches=20):\n",
    "    errs=[]\n",
    "    for _ in range(batches):\n",
    "        z,t,y = make_synth_batch()\n",
    "        z_new = torch.randn_like(z[:,:1,:])\n",
    "        Zin = torch.cat([z, z_new], 1)\n",
    "        Tin1 = torch.cat([t, torch.ones_like(t[:,:1,:])], 1)\n",
    "        Tin0 = torch.cat([t, torch.zeros_like(t[:,:1,:])], 1)\n",
    "        Yin = torch.zeros_like(Tin1)\n",
    "        y1 = model(Zin, Tin1, Yin)[:,-1,0]\n",
    "        y0 = model(Zin, Tin0, Yin)[:,-1,0]\n",
    "        tau_hat = y1 - y0\n",
    "        tau_true = torch.full_like(tau_hat, cfg.SCM_EFFECT)\n",
    "        errs.append(((tau_hat - tau_true)**2).mean().sqrt().item())\n",
    "    return np.mean(errs)\n",
    "\n",
    "pehe_s = pehe(model, batches=20)\n",
    "print(\"PEHE (synth):\", pehe_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ee0c8b-756c-4dda-95c4-c9e229bc9b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Baseline ingenuo (regresión lineal) en Z -> Y_mock (no causal)\n",
    "lin = LinearRegression().fit(Z, Y_mock)\n",
    "pred_lin = lin.predict(Z)\n",
    "print(\"Baseline Lin MSE:\", mean_squared_error(Y_mock, pred_lin),\n",
    "      \"R2:\", r2_score(Y_mock, pred_lin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aebe960-9076-4bb6-b396-8f348c0702c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexión con Z Reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e9f046-2e57-4c58-b2e7-d55b0589aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% CATE en latentes REALES (ISLES) con el modelo causal entrenado\n",
    "@torch.no_grad()\n",
    "def predict_cate_real(model, Z_real_np, batch_size=16):\n",
    "    model.eval()\n",
    "    Zr = torch.from_numpy(Z_real_np).float().to(device)  # Z reales del VAE\n",
    "    N, F = Zr.shape\n",
    "    cates = []\n",
    "    for i in range(0, N, batch_size):\n",
    "        z_batch = Zr[i:i+batch_size]                     # [B,F]\n",
    "        # Tratamiento observacional sintético (si no tienes T_real)\n",
    "        logits_t = z_batch[:,0]*2.0 + torch.randn_like(z_batch[:,0])\n",
    "        prob_t = torch.sigmoid(logits_t)\n",
    "        t_obs = torch.bernoulli(prob_t).unsqueeze(-1)    # [B,1]\n",
    "\n",
    "        # Secuencia mínima (contexto + query)\n",
    "        z_seq = torch.stack([z_batch, z_batch], dim=1)   # [B,2,F]\n",
    "        t_ctx = t_obs.unsqueeze(1)                       # [B,1,1]\n",
    "        t_q1 = torch.ones_like(t_ctx)                    # do(T=1)\n",
    "        t_q0 = torch.zeros_like(t_ctx)                   # do(T=0)\n",
    "\n",
    "        def run_with(t_query):\n",
    "            Zin = torch.cat([z_seq[:,0:1,:], z_seq[:,1:2,:]], dim=1)\n",
    "            Tin = torch.cat([t_ctx, t_query], dim=1)\n",
    "            Yin = torch.zeros_like(Tin)\n",
    "            out = model(Zin, Tin, Yin)\n",
    "            return out[:, -1, 0]\n",
    "\n",
    "        y1 = run_with(t_q1)\n",
    "        y0 = run_with(t_q0)\n",
    "        cates.append((y1 - y0).cpu())\n",
    "    return torch.cat(cates, dim=0).numpy()\n",
    "\n",
    "cate_real = predict_cate_real(model, Z)\n",
    "print(\"CATE real (latentes ISLES) mean:\", cate_real.mean(), \"std:\", cate_real.std())\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(cate_real, bins=20, color='skyblue', edgecolor='k')\n",
    "plt.axvline(cate_real.mean(), color='r', linestyle='--', label=f\"Media={cate_real.mean():.2f}\")\n",
    "plt.title(\"Distribución CATE en latentes ISLES\")\n",
    "plt.xlabel(\"CATE\"); plt.ylabel(\"Frecuencia\"); plt.legend(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62e4ada-080f-447e-a37f-b0f2283a29a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Evaluación híbrida (Z reales + SCM labels)\n",
    "@torch.no_grad()\n",
    "def eval_hybrid(model, Z_real_np, batches=10):\n",
    "    Zr = torch.from_numpy(Z_real_np).float().to(device)\n",
    "    errs=[]\n",
    "    for _ in range(batches):\n",
    "        z,t,y = make_synth_batch(on_real_Z=Zr)\n",
    "        z_new = torch.randn_like(z[:,:1,:])\n",
    "        Zin = torch.cat([z, z_new], 1)\n",
    "        Tin1 = torch.cat([t, torch.ones_like(t[:,:1,:])], 1)\n",
    "        Tin0 = torch.cat([t, torch.zeros_like(t[:,:1,:])], 1)\n",
    "        Yin = torch.zeros_like(Tin1)\n",
    "        y1 = model(Zin, Tin1, Yin)[:,-1,0]\n",
    "        y0 = model(Zin, Tin0, Yin)[:,-1,0]\n",
    "        tau_hat = y1 - y0\n",
    "        tau_true = torch.full_like(tau_hat, cfg.SCM_EFFECT)\n",
    "        errs.append(((tau_hat - tau_true)**2).mean().sqrt().item())\n",
    "    return np.mean(errs)\n",
    "\n",
    "pehe_h = eval_hybrid(model, Z, batches=10)\n",
    "print(\"PEHE híbrido (Z reales + SCM):\", pehe_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8880751-b7df-4083-b2ee-50f2bc07e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparación observacional vs causal (Do-PFN)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Batch sintético con el mismo SCM\n",
    "    z_batch, t_batch, y_batch = make_synth_batch(\n",
    "        batch_size=cfg.SYN_BATCH,\n",
    "        seq_len=cfg.SYN_SEQ,\n",
    "        n_features=cfg.LATENT_DIM\n",
    "    )\n",
    "    t_flat = t_batch.flatten()\n",
    "    y_flat = y_batch.flatten()\n",
    "\n",
    "    obs_effect = y_flat[t_flat == 1].mean() - y_flat[t_flat == 0].mean()  # sesgado por Z\n",
    "    causal_effect = predict_cate(model, z_batch, t_batch)                # do-effect ~2.0\n",
    "\n",
    "print(f\"Efecto OBSERVACIONAL (sesgado): {obs_effect:.3f}\")\n",
    "print(f\"Efecto CAUSAL (Do-PFN):        {causal_effect:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd31cd5-9504-4868-8805-b1fa967ada3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización del confounding: Z→T y T↔Y (espurio vs causal)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "z0 = z_batch[:, :, 0].flatten().cpu()\n",
    "t_flat = t_flat.cpu()\n",
    "y_flat = y_flat.cpu()\n",
    "\n",
    "# Z[0] → T (propensión confounded)\n",
    "axes[0].scatter(z0, t_flat, alpha=0.4, edgecolors=\"k\")\n",
    "axes[0].set_xlabel(\"Z[0]\")\n",
    "axes[0].set_ylabel(\"T\")\n",
    "axes[0].set_title(\"Asignación de tratamiento sesgada por Z[0]\")\n",
    "\n",
    "# T ↔ Y (correlación espuria)\n",
    "axes[1].scatter(t_flat + 0.02 * torch.randn_like(t_flat), y_flat, alpha=0.4, edgecolors=\"k\")\n",
    "axes[1].set_xlabel(\"T\")\n",
    "axes[1].set_ylabel(\"Y observado\")\n",
    "axes[1].set_title(\"Correlación T–Y (mezcla de efecto causal + confounding)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b95280-9c22-4497-bb1c-d198c89349fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    from causalml.dataset import synthetic_data  \n",
    "    X, y, t, b, e = synthetic_data(mode=1, n=2000, p=20, sigma=1.0) \n",
    "\n",
    "    # Normaliza shapes para reutilizar el modelo (Z = features)\n",
    "    Z_ext = torch.from_numpy(X).float().to(device)\n",
    "    T_ext = torch.from_numpy(t).float().unsqueeze(-1).to(device)\n",
    "    # Outcome observado\n",
    "    Y_ext = torch.from_numpy(y).float().unsqueeze(-1).to(device)\n",
    "\n",
    "    def predict_cate_tab(model, Z_tensor, T_tensor, batch_size=64):\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, Z_tensor.size(0), batch_size):\n",
    "                zb = Z_tensor[i:i+batch_size].unsqueeze(1)          # [B,1,F]\n",
    "                tb = T_tensor[i:i+batch_size].unsqueeze(1)          # [B,1,1]\n",
    "                # dummy Y (mascarado en forward), usa ceros\n",
    "                y_masked = torch.zeros_like(tb).to(device)\n",
    "                # do(T=1) y do(T=0)\n",
    "                y1 = model(zb, torch.ones_like(tb), y_masked)\n",
    "                y0 = model(zb, torch.zeros_like(tb), y_masked)\n",
    "                preds.append((y1 - y0).cpu())\n",
    "        return torch.cat(preds, dim=0)\n",
    "\n",
    "    tau_hat_ext = predict_cate_tab(model, Z_ext, T_ext)\n",
    "    print(f\"CATE medio (externo): {tau_hat_ext.mean().item():.3f}\")\n",
    "except ImportError as e:\n",
    "    print(\" RealCause/ACIC loader no disponible. Instala el dataset y ajusta el stub.\")\n",
    "except Exception as e:\n",
    "    print(\" Benchmark externo saltado por error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb3fbf3-566c-4e9c-b9ed-d3e0e574a31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Guardar artefactos\n",
    "os.makedirs(\"Data\", exist_ok=True)\n",
    "np.save(\"Data/latents.npy\", Z)\n",
    "np.save(\"Data/y_mock.npy\", Y_mock)\n",
    "torch.save(vae.state_dict(), \"Data/vae_checkpoint.pt\")\n",
    "torch.save(model.state_dict(), \"Data/tabicl_mini.pt\")\n",
    "print(\"Artefactos guardados.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-debugging]",
   "language": "python",
   "name": "conda-env-anaconda3-debugging-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
