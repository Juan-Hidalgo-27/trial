{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9b15380-37e8-43c4-b61d-3a5f4d1c05ef",
   "metadata": {},
   "source": [
    "# Neuro-TabPFN v0.3 - Pipeline (Colab-ready)\n",
    "\n",
    "Objetivo: usar **solo** el pipeline del repo `high-dimensional/individualized_prescriptive_inference` con sus embeddings (PCA/NMF **y VAE propio del repo**), manteniendo la resolución nativa MNI **91×109×91** y usando **todas las 4119 máscaras**.\n",
    "\n",
    "Flujo:\n",
    "\n",
    "**Resumen del flujo:**\n",
    "1. Setup de entorno y rutas\n",
    "2. Instalación de dependencias + clonación repo\n",
    "3. Descarga/lectura de 4119 máscaras (lesiones) \n",
    "4. Representación (genera embeddings AE/VAE/NMF/PCA en k-folds)\n",
    "5. Deficit modelling: `deficit_modelling.py`\n",
    "6. Prescriptive simulations: `prescription.py` con `--use_vae True`\n",
    "7. TabICL Two-Stage + Do-Loss training + evaluación\n",
    "8. Guardado de salidas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05291b1c-2bd6-4edc-b248-c671efca1f06",
   "metadata": {},
   "source": [
    "## 1. Setup de entorno y rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c92bea4-406d-4b5e-a23b-b62ede1650d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dir: H:/My Drive\\Debbuging Neuro\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Si usas Drive, pon USE_DRIVE=True; de lo contrario change \"H:/My Drive\" por tu directorio local\n",
    "USE_DRIVE = False\n",
    "DRIVE_ROOT = \"/content/drive/MyDrive\" if USE_DRIVE else \"H:/My Drive\"\n",
    "\n",
    "ROOT = os.path.join(DRIVE_ROOT)\n",
    "ROOT_DIR = os.path.join(ROOT, \"Debbuging Neuro\")\n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"Data\")\n",
    "RESULTS_DIR = os.path.join(ROOT_DIR, \"Results\")\n",
    "REPO_DIR = os.path.join(ROOT, \"individualized_prescriptive_inference\")\n",
    "REPO_RESULTS = os.path.join(REPO_DIR, \"results\")\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(REPO_RESULTS, exist_ok=True)\n",
    "\n",
    "os.chdir(ROOT)\n",
    "    \n",
    "print(f\"Working dir: {ROOT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e90fb9d1-fe9f-4baa-b958-ac236129a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r requirements.txt\n",
    "#!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a04902f7-88d7-4581-a057-5f3e952a7d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import glob\n",
    "import zipfile\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nibabel as nib\n",
    "from scipy.ndimage import zoom\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from nilearn import datasets, image\n",
    "import urllib\n",
    "import os\n",
    "import shutil\n",
    "import gzip\n",
    "import sys\n",
    "import json, random, warnings, textwrap, subprocess\n",
    "from typing import Tuple\n",
    "import subprocess\n",
    "import sys\n",
    "import textwrap\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    import torch_directml\n",
    "    dml = torch_directml.device()\n",
    "except ImportError:\n",
    "    dml = None\n",
    "    \n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e33aaaed-da02-4da8-a10e-a6b1074f3726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: privateuseone:0\n",
      "Latent dim: 50, K-folds: 10\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIG \n",
    "# =============================================================================\n",
    "# Si usas mac descomentar el primer device y comentar el segundo device\n",
    "\n",
    "class Config:\n",
    "    SEED = 42\n",
    "    \n",
    "    # Device detection\n",
    "    # DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    try:\n",
    "        import torch_directml\n",
    "        DEVICE = torch_directml.device()\n",
    "    except ImportError:\n",
    "        DEVICE = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else\n",
    "            \"mps\" if torch.backends.mps.is_available() else\n",
    "            \"cpu\"\n",
    "        )\n",
    "    # Trial\n",
    "    SAMPLES = 10\n",
    "    \n",
    "    # Pipeline params (usados por representation.py)\n",
    "    TARGET_SHAPE = (91, 109, 91)\n",
    "    LATENT_DIM = 50\n",
    "    K_FOLDS = 10\n",
    "    \n",
    "    # TabICL params (solo si usas TabICL después)\n",
    "    D_MODEL = 128\n",
    "    N_HEAD = 4\n",
    "    N_LAYERS_COL = 2\n",
    "    N_LAYERS_ROW = 4\n",
    "    DIM_FEEDFORWARD = 512\n",
    "    DROPOUT = 0.1\n",
    "    LR_TABICL = 5e-4\n",
    "    SYN_BATCH = 64\n",
    "    SYN_SEQ = 48\n",
    "    DO_STEPS = 400\n",
    "    \n",
    "    # SCM params para simulación causal\n",
    "    SCM_EFFECT = 5.0\n",
    "    SCM_WEIGHT_Z = 0.5\n",
    "    SCM_WEIGHT_T = 2.0\n",
    "\n",
    "cfg = Config()\n",
    "random.seed(cfg.SEED)\n",
    "np.random.seed(cfg.SEED)\n",
    "torch.manual_seed(cfg.SEED)\n",
    "\n",
    "print(f\"Device: {cfg.DEVICE}\")\n",
    "print(f\"Latent dim: {cfg.LATENT_DIM}, K-folds: {cfg.K_FOLDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1937e2c-6796-4645-b314-8f441beec6c0",
   "metadata": {},
   "source": [
    "## 2. Instalación de dependencias + clonación repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515c049c-4c9a-4288-ab69-cdfb07f9cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# CLONAR REPO + REQS\n",
    "%cd \"$ROOT\"\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone https://github.com/high-dimensional/individualized_prescriptive_inference.git \"$REPO_DIR\"\n",
    "%cd $REPO_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf59380-29b7-4cb8-aee9-217cfa6180af",
   "metadata": {},
   "source": [
    "## 3. Descarga de 4119 máscaras (lesiones) + Otros dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9640cc6-999f-4053-8d4a-c09d614befe3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DESCARGA Y PREPROCESADO DE MÁSCARAS\n",
    "import os, zipfile, glob, shutil\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "LESIONS_ZIP = os.path.join(REPO_DIR, \"lesions.zip\")\n",
    "URL = \"https://github.com/high-dimensional/individualized_prescriptive_inference/raw/main/lesions.zip\"\n",
    "\n",
    "# Descargar\n",
    "if not os.path.exists(LESIONS_ZIP):\n",
    "    try:\n",
    "        urlretrieve(URL, LESIONS_ZIP)\n",
    "    except Exception:\n",
    "        import subprocess\n",
    "        subprocess.run([\"wget\", \"-q\", URL, \"-O\", LESIONS_ZIP], check=True)\n",
    "        \n",
    "# Descomprimir directamente en REPO_DIR\n",
    "if not glob.glob(os.path.join(REPO_DIR, \"*.nii*\")):\n",
    "    try:\n",
    "        with zipfile.ZipFile(LESIONS_ZIP, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(REPO_DIR)\n",
    "    except Exception:\n",
    "        import subprocess\n",
    "        subprocess.run([\"unzip\", \"-q\", LESIONS_ZIP, \"-d\", REPO_DIR], check=True)\n",
    "\n",
    "# Verificar\n",
    "LESIONES_PATH = os.path.join(REPO_DIR, \"lesions\")\n",
    "DISCO_PATH = os.path.join(REPO_DIR, \"disconnectomes\")\n",
    "\n",
    "lesion_files = sorted(glob.glob(os.path.join(LESIONES_PATH, \"*.nii*\")))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Total máscaras:\", len(lesion_files))\n",
    "\n",
    "# Recorrer los primeros archivos para verificar su forma (shape)\n",
    "for f in lesion_files[:3]: \n",
    "    img = nib.load(f)\n",
    "    print(f\"Archivo: {os.path.basename(f)}\")\n",
    "    print(f\" - Shape: {img.shape}\")\n",
    "    print(f\" - Affine (orientación):\\n{img.affine}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65114e1b-5943-45d9-afd1-a17818d71495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas directory exists at H:/My Drive\\individualized_prescriptive_inference\\atlases\n",
      "Already exists: functional_parcellation_2mm.nii\n",
      "Already exists: icv_mask_2mm.nii\n",
      "all_territories.nii: (91, 109, 91)\n",
      "major_arterial_territory.nii: (91, 109, 91)\n",
      "major_arterial_territory_lat.nii: labels [ 0  1  3  5  7  9 11 13]\n",
      "major_territories.nii: anterior=50671, posterior=116476 voxels\n",
      "\n",
      "Atlas preprocessing complete.\n",
      "\n",
      "FILE                                          | SHAPE           | STATE\n",
      "---------------------------------------------------------------------------\n",
      "all_territories.nii                           | (91, 109, 91)   | OK\n",
      "major_arterial_territory.nii                  | (91, 109, 91)   | OK\n",
      "major_arterial_territory_lat.nii              | (91, 109, 91)   | OK\n",
      "major_territories.nii                         | (91, 109, 91)   | OK\n",
      "functional_parcellation_2mm.nii               | (91, 109, 91)   | OK\n",
      "icv_mask_2mm.nii                              | (91, 109, 91)   | OK\n"
     ]
    }
   ],
   "source": [
    "# Atlas Preprocessing for Stroke Lesion Analysis\n",
    "# References: Liu et al. (2023). Digital 3D brain MRI arterial territories atlas. Scientific Data, 10(1), 1-12. https://doi.org/10.1038/s41597-022-01923-0\n",
    "\n",
    "ATLAS_DIR = os.path.join(REPO_DIR, \"atlases\")\n",
    "VASC_ATLAS_DIR = os.path.join(ATLAS_DIR, \"vasc_atlas\")\n",
    "NITRC_ATLAS_DIR = os.path.join(DATA_DIR, \"NITRC\")\n",
    "\n",
    "COMPRESSED_FILES = [\n",
    "    \"functional_parcellation_2mm.nii.gz\",\n",
    "    \"icv_mask_2mm.nii.gz\"\n",
    "]\n",
    "\n",
    "# =============================================================================\n",
    "# Utility Functions\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_resample_to_lesion_space(atlas_path: str, reference_path: str) -> Tuple[nib.Nifti1Image, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load NIfTI atlas and resample to match lesion mask space exactly.\n",
    "    Handles 4D inputs by taking first volume.\n",
    "    \"\"\"\n",
    "    atlas_img = nib.load(atlas_path)\n",
    "    atlas_data = atlas_img.get_fdata()\n",
    "    \n",
    "    # Handle 4D: take first volume\n",
    "    if atlas_data.ndim == 4:\n",
    "        atlas_data = atlas_data[:, :, :, 0]\n",
    "        atlas_img = nib.Nifti1Image(atlas_data, atlas_img.affine)\n",
    "    \n",
    "    # Load reference (lesion or ICV mask) for target space\n",
    "    ref_img = nib.load(reference_path)\n",
    "    \n",
    "    # Resample atlas to reference space\n",
    "    resampled = image.resample_to_img(\n",
    "        atlas_img, \n",
    "        ref_img, \n",
    "        interpolation=\"nearest\"\n",
    "    )\n",
    "    \n",
    "    return resampled, resampled.get_fdata().astype(np.int16), resampled.affine\n",
    "\n",
    "\n",
    "def lateralize_atlas(data: np.ndarray, n_labels: int = 4) -> np.ndarray:\n",
    "    \"\"\"Split atlas by hemisphere: left=[1,n], right=[n+1,2n].\"\"\"\n",
    "    mid = data.shape[0] // 2\n",
    "    out = np.zeros_like(data)\n",
    "    out[:mid] = data[:mid]\n",
    "    out[mid:] = np.where(data[mid:] > 0, data[mid:] + n_labels, 0)\n",
    "    return out\n",
    "\n",
    "\n",
    "def classify_circulation(data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Map territories to anterior (1) vs posterior (2) circulation.\"\"\"\n",
    "    out = np.zeros_like(data)\n",
    "    out[np.isin(data, [1, 2])] = 1  # ACA + MCA\n",
    "    out[np.isin(data, [3, 4])] = 2  # PCA + VB\n",
    "    return out\n",
    "\n",
    "# =============================================================================\n",
    "# Step 1: Sync Atlas Directory from Repository\n",
    "# =============================================================================\n",
    "\n",
    "source_atlas = os.path.join(REPO_DIR, \"atlases\")\n",
    "\n",
    "if os.path.exists(source_atlas) and not os.path.exists(ATLAS_DIR):\n",
    "    shutil.copytree(source_atlas, ATLAS_DIR)\n",
    "    print(f\"Synced atlas directory to {ATLAS_DIR}\")\n",
    "elif not os.path.exists(source_atlas):\n",
    "    os.makedirs(ATLAS_DIR, exist_ok=True)\n",
    "    print(f\"Created empty atlas directory at {ATLAS_DIR}\")\n",
    "else:\n",
    "    print(f\"Atlas directory exists at {ATLAS_DIR}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Step 2: Decompress Required Files\n",
    "# =============================================================================\n",
    "\n",
    "for gz_name in COMPRESSED_FILES:\n",
    "    gz_path = os.path.join(ATLAS_DIR, gz_name)\n",
    "    nii_path = os.path.join(ATLAS_DIR, gz_name.replace(\".gz\", \"\"))\n",
    "    \n",
    "    if os.path.exists(gz_path) and not os.path.exists(nii_path):\n",
    "        with gzip.open(gz_path, 'rb') as f_in, open(nii_path, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "        print(f\"Decompressed: {gz_name}\")\n",
    "    elif os.path.exists(nii_path):\n",
    "        print(f\"Already exists: {os.path.basename(nii_path)}\")\n",
    "    else:\n",
    "        print(f\"WARNING: Missing source file {gz_name}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Step 3: Build Vascular Territory Atlases (resampled to lesion space)\n",
    "# =============================================================================\n",
    "\n",
    "os.makedirs(VASC_ATLAS_DIR, exist_ok=True)\n",
    "\n",
    "# Reference image for target space (ICV mask has correct shape)\n",
    "REFERENCE_PATH = os.path.join(ATLAS_DIR, \"icv_mask_2mm.nii\")\n",
    "\n",
    "if not os.path.exists(REFERENCE_PATH):\n",
    "    print(f\"ERROR: Reference image not found: {REFERENCE_PATH}\")\n",
    "else:\n",
    "    # 3.1 Full parcellation (30 territories)\n",
    "    full_atlas_path = os.path.join(NITRC_ATLAS_DIR, \"ArterialAtlas.nii\")\n",
    "    if os.path.exists(full_atlas_path):\n",
    "        full_img, _, _ = load_and_resample_to_lesion_space(full_atlas_path, REFERENCE_PATH)\n",
    "        nib.save(full_img, os.path.join(VASC_ATLAS_DIR, \"all_territories.nii\"))\n",
    "        print(f\"all_territories.nii: {full_img.shape}\")\n",
    "    else:\n",
    "        print(f\"WARNING: {full_atlas_path} not found\")\n",
    "\n",
    "    # 3.2 Major territories (ACA, MCA, PCA, VB)\n",
    "    level2_path = os.path.join(NITRC_ATLAS_DIR, \"ArterialAtlas_level2.nii\")\n",
    "    if os.path.exists(level2_path):\n",
    "        level2_img, level2_data, affine = load_and_resample_to_lesion_space(level2_path, REFERENCE_PATH)\n",
    "        nib.save(level2_img, os.path.join(VASC_ATLAS_DIR, \"major_arterial_territory.nii\"))\n",
    "        print(f\"major_arterial_territory.nii: {level2_img.shape}\")\n",
    "        \n",
    "        # 3.3 Lateralized version (L/R hemisphere split)\n",
    "        lat_data = lateralize_atlas(level2_data, n_labels=4)\n",
    "        nib.save(nib.Nifti1Image(lat_data, affine), os.path.join(VASC_ATLAS_DIR, \"major_arterial_territory_lat.nii\"))\n",
    "        print(f\"major_arterial_territory_lat.nii: labels {np.unique(lat_data)}\")\n",
    "        \n",
    "        # 3.4 Anterior vs posterior circulation\n",
    "        circ_data = classify_circulation(level2_data)\n",
    "        nib.save(nib.Nifti1Image(circ_data, affine), os.path.join(VASC_ATLAS_DIR, \"major_territories.nii\"))\n",
    "        print(f\"major_territories.nii: anterior={np.sum(circ_data==1)}, posterior={np.sum(circ_data==2)} voxels\")\n",
    "    else:\n",
    "        print(f\"WARNING: {level2_path} not found\")\n",
    "\n",
    "print(\"\\nAtlas preprocessing complete.\")\n",
    "\n",
    "# =============================================================================\n",
    "# Step 4: Verify all shapes match\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n{'FILE':<45} | {'SHAPE':<15} | {'STATE'}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "archivos_a_verificar = [\n",
    "    os.path.join(VASC_ATLAS_DIR, \"all_territories.nii\"),\n",
    "    os.path.join(VASC_ATLAS_DIR, \"major_arterial_territory.nii\"),\n",
    "    os.path.join(VASC_ATLAS_DIR, \"major_arterial_territory_lat.nii\"),\n",
    "    os.path.join(VASC_ATLAS_DIR, \"major_territories.nii\"),\n",
    "    os.path.join(ATLAS_DIR, \"functional_parcellation_2mm.nii\"),\n",
    "    os.path.join(ATLAS_DIR, \"icv_mask_2mm.nii\")\n",
    "]\n",
    "\n",
    "for path in archivos_a_verificar:\n",
    "    nombre = os.path.basename(path)\n",
    "    if os.path.exists(path):\n",
    "        img = nib.load(path)\n",
    "        shape = img.shape\n",
    "        es_compatible = \"OK\" if shape == cfg.TARGET_SHAPE else \"DISCREPANCIA\"\n",
    "        print(f\"{nombre:<45} | {str(shape):<15} | {es_compatible}\")\n",
    "    else:\n",
    "        print(f\"{nombre:<45} | {'NO ENCONTRADO':<15} | REVISAR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11281159-b039-4cb1-9f08-5e5a48c25927",
   "metadata": {},
   "source": [
    "## 4. Representación (genera embeddings NMF/PCA en k-folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22454fe2-fd4c-4304-a28b-719c8f699a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:\\My Drive\\individualized_prescriptive_inference\n",
      "Executing: C:\\ProgramData\\anaconda3\\envs\\debugging\\python.exe H:/My Drive\\individualized_prescriptive_inference\\software\\representation.py --lesionpath H:/My Drive\\individualized_prescriptive_inference\\lesions --discopath H:/My Drive\\individualized_prescriptive_inference\\disconnectomes --savepath H:/My Drive\\individualized_prescriptive_inference\\results\\representations --kfolds 10 --latent_components 50 --batch_size 32 --min_epoch 16 --max_epoch 32 --early_stopping_epochs 4 --n_samples 100 --run_vae True --run_ae False --run_nmf False --run_pca False --verbose True\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Representation Learning Module Execution\n",
    "========================================\n",
    "Executes dimensionality reduction pipeline (PCA, NMF, VAE) on lesion masks.\n",
    "\n",
    "This step generates latent embeddings for downstream causal inference.\n",
    "See Giles et al. (2025), Section 5.2 for methodology details.\n",
    "\"\"\"\n",
    "%cd {REPO_DIR}\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration\n",
    "# =============================================================================\n",
    "\n",
    "REPRESENTATION_SCRIPT = os.path.join(REPO_DIR, \"software\", \"representation.py\")\n",
    "OUTPUT_DIR = os.path.join(REPO_RESULTS, \"representations\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# Build Command \n",
    "# =============================================================================\n",
    "cmd_args = [\n",
    "    sys.executable,\n",
    "    REPRESENTATION_SCRIPT,\n",
    "    \"--lesionpath\", LESIONES_PATH,\n",
    "    \"--discopath\", DISCO_PATH,  \n",
    "    \"--savepath\", OUTPUT_DIR,\n",
    "    \"--kfolds\", \"10\",\n",
    "    \"--latent_components\", \"50\",\n",
    "    \"--batch_size\", \"32\",\n",
    "    \"--min_epoch\", \"16\",\n",
    "    \"--max_epoch\", \"32\",\n",
    "    \"--early_stopping_epochs\", \"4\",\n",
    "    \"--n_samples\", \"100\",\n",
    "    \"--run_vae\", \"True\",   \n",
    "    \"--run_ae\", \"False\",   \n",
    "    \"--run_nmf\", \"False\",\n",
    "    \"--run_pca\", \"False\",\n",
    "    \"--verbose\", \"True\"\n",
    "]\n",
    "\n",
    "print(f\"Executing: {' '.join(cmd_args)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# =============================================================================\n",
    "# Execute\n",
    "# =============================================================================\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        cmd_args,\n",
    "        check=True,\n",
    "        capture_output=False,\n",
    "        text=True,\n",
    "        env={**os.environ, \"PYTHONUNBUFFERED\": \"1\"}\n",
    "    )\n",
    "    print(\"Execution completed successfully.\")\n",
    "    if result.stdout:\n",
    "        print(result.stdout)\n",
    "        \n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"\\nProcess failed with exit code {e.returncode}\")\n",
    "    print(\"-\" * 60)\n",
    "    error_output = e.stderr or e.stdout\n",
    "    if error_output:\n",
    "        lines = error_output.strip().split('\\n')\n",
    "        traceback_start = next((i for i, l in enumerate(lines) if 'Traceback' in l), 0)\n",
    "        print('\\n'.join(lines[traceback_start:]))\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1568c219-cd55-4ced-8ddf-e2b5361bf60d",
   "metadata": {},
   "source": [
    "## 5. Deficit modelling: `deficit_modelling.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afebe94-1859-45bc-af72-3acf43826152",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deficit Modelling Execution\n",
    "===========================\n",
    "Maps lesion representations to functional network disruptions.\n",
    "\n",
    "Computes overlap between lesion masks and 16 NeuroQuery functional networks,\n",
    "generating binary deficit labels for downstream prescriptive inference.\n",
    "\n",
    "References:\n",
    "    Giles et al. (2025), Section: Deficit Modelling\n",
    "    Dockès et al. (2020), NeuroQuery functional parcellation\n",
    "\"\"\"\n",
    "\n",
    "%cd {REPO_DIR}\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration\n",
    "# =============================================================================\n",
    "\n",
    "DEFICIT_SCRIPT = os.path.join(REPO_DIR, \"software\", \"deficit_modelling.py\")\n",
    "OUTPUT_DIR = os.path.join(REPO_RESULTS, \"representations\")\n",
    "\n",
    "# Cross-validation and threshold parameters\n",
    "\n",
    "ROI_THRESHOLD = 0.05  # Minimum overlap fraction to mark network as affected\n",
    "thresh_str = str(ROI_THRESHOLD) if not isinstance(ROI_THRESHOLD, list) else str(ROI_THRESHOLD[0])\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Build Command\n",
    "# =============================================================================\n",
    "\n",
    "cmd_args = [\n",
    "    sys.executable,\n",
    "    DEFICIT_SCRIPT,\n",
    "    \"--path\", OUTPUT_DIR,\n",
    "    \"--lesionpath\", LESIONES_PATH,\n",
    "    \"--discopath\", DISCO_PATH,  \n",
    "    \"--latent_list\", \"50\",\n",
    "    \"--kfold_deficits\", \"10\",\n",
    "    \"--roi_threshs\", thresh_str,\n",
    "    \"--run_vae\", \"True\",   \n",
    "    \"--run_ae\", \"False\",   \n",
    "    \"--run_nmf\", \"False\",\n",
    "    \"--run_pca\", \"False\",\n",
    "    #\"--n_samples\", \"10\", \n",
    "    \"--verbose\", \"True\" \n",
    "]\n",
    "\n",
    "\n",
    "print(f\"Executing deficit modelling with {len(LATENT_DIMS)} latent dimensions\")\n",
    "print(f\"Parameters: K={K_FOLDS}, threshold={ROI_THRESHOLD}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Execute with Full Error Capture\n",
    "# =============================================================================\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        cmd_args,\n",
    "        check=True,\n",
    "        capture_output=False,\n",
    "        text=True,\n",
    "        env={**os.environ, \"PYTHONUNBUFFERED\": \"1\"}\n",
    "    )\n",
    "    print(\"Deficit modelling completed successfully.\")\n",
    "    if result.stdout:\n",
    "        print(result.stdout)\n",
    "\n",
    "except subprocess.CalledProcessError as e:\n",
    "    error_output = e.stderr or e.stdout\n",
    "    if error_output:\n",
    "        lines = error_output.strip().split('\\n')\n",
    "        traceback_start = next((i for i, l in enumerate(lines) if 'Traceback' in l), 0)\n",
    "        print('\\n'.join(lines[traceback_start:]))\n",
    "    else:\n",
    "        print(\"No error output captured.\")\n",
    "    \n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f61fb76-2af5-48e1-ad5c-16418975a36e",
   "metadata": {},
   "source": [
    "## 6. Prescriptive simulations: `prescription.py` con `--use_vae True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd45c2c-f346-44ef-bc3d-69035045ce43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prescriptive Inference Simulations\n",
    "==================================\n",
    "Executes virtual clinical trials to evaluate treatment effect estimation\n",
    "under varying confounding scenarios.\n",
    "\n",
    "This module implements the InterSynth evaluation framework, testing\n",
    "prescriptive models across 22,528 discrete DGP configurations.\n",
    "\n",
    "References:\n",
    "    Giles et al. (2025), Nature Communications - Sections: Virtual Trials, Prescriptive Inference\n",
    "    \n",
    "Parameters:\n",
    "    - biasdegree (gamma): Confounding strength [0, 1]\n",
    "    - te (beta): Treatment effect magnitude\n",
    "    - re (alpha): Spontaneous recovery rate\n",
    "    - deficits: 16 functional networks from NeuroQuery\n",
    "\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration\n",
    "# =============================================================================\n",
    "\n",
    "PRESCRIPTION_SCRIPT = os.path.join(REPO_DIR, \"software\", \"prescription.py\")\n",
    "PRESCRIPTION_DIR = os.path.join(REPO_RESULTS, \"prescription\")\n",
    "REPRESENTATIONS_DIR = os.path.join(REPO_RESULTS, \"representations\")\n",
    "\n",
    "thresh_str = str(ROI_THRESHOLD) if not isinstance(ROI_THRESHOLD, list) else str(ROI_THRESHOLD[0])\n",
    "\n",
    "# =============================================================================\n",
    "# Build Command\n",
    "# =============================================================================\n",
    "\n",
    "cmd_args = [\n",
    "    sys.executable,\n",
    "    PRESCRIPTION_SCRIPT,\n",
    "    \"--savepath\", PRESCRIPTION_DIR,     # Where to save final results\n",
    "    \"--loadpath\", REPRESENTATIONS_DIR,  # Where to load data (Deficit Modelling Output)\n",
    "    \n",
    "    # Experiment Configuration\n",
    "    \"--k\", *[str(k) for k in range(10)],\n",
    "    \"--gene_or_receptor\", \"genetics\", \"receptor\",\n",
    "    \"--lesion_or_disconnectome\", \"lesion\", \"disco\", \n",
    "    \"--lesion_deficit_thresh\", thresh_str, thresh_str,\n",
    "    \n",
    "    # Simulation Parameters (Reduced for debug)\n",
    "    \"--deficits\", \"10\", \"11\",         \n",
    "    \"--biasdegree\", \"0\", \"0.5\",     \n",
    "    \"--biastype\", \"observed\",        \n",
    "    \"--te\", \"0.5\",                  \n",
    "    \"--re\", \"0.25\",                 \n",
    "    \n",
    "    # Models and Representations\n",
    "    \"--bottlenecks\", \"0\", \"50\",     \n",
    "    \"--simpleatlases\", \"major_territories\", \n",
    "    \"--simpleatlas_argmaxs\", \"True\",\n",
    "    \"--vols\", \"True\",\n",
    "    \"--centroids\", \"False\",\n",
    "    \"--ml_models\", \"xgb\", \n",
    "    \n",
    "    # Which embeddings to use (Matches your previous steps)\n",
    "    \"--use_vae\", \"True\",    \n",
    "    \"--use_ae\", \"False\",    \n",
    "    \"--use_nmf\", \"False\",\n",
    "    \"--use_pca\", \"False\"\n",
    "]\n",
    "\n",
    "\n",
    "n_configs = (\n",
    "    10 * 2 * 2 * # Folds * Deficits * Subdivisions\n",
    "    2 * 2 * 1 * 1 # Inputs * Bias * TE * RE\n",
    ")\n",
    "print(f\" Ejecutando Prescription para los 10 folds...\")\n",
    "print(f\"   Inputs: lesion + disco\")\n",
    "print(f\"   Input Dir: {REPRESENTATIONS_DIR}\")\n",
    "print(f\"   Output Dir: {PRESCRIPTION_DIR}\")\n",
    "print(f\"   Thresholds: {thresh_str}, {thresh_str}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Execute with Full Error Capture\n",
    "# =============================================================================\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        cmd_args,\n",
    "        check=True,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        env={**os.environ, \"PYTHONUNBUFFERED\": \"1\"}\n",
    "    )\n",
    "    print(\"Prescriptive simulations completed successfully.\")\n",
    "    if result.stdout:\n",
    "        print(result.stdout[-2000:])  # Last 2000 chars to avoid overflow\n",
    "\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"\\nProcess failed with exit code {e.returncode}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    error_output = e.stderr or e.stdout\n",
    "    if error_output:\n",
    "        lines = error_output.strip().split('\\n')\n",
    "        traceback_start = next((i for i, l in enumerate(lines) if 'Traceback' in l), 0)\n",
    "        print('\\n'.join(lines[traceback_start:]))\n",
    "    else:\n",
    "        print(\"No error output captured.\")\n",
    "    \n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360f7f9f-c882-4fbf-ba6a-f5292f9e3e94",
   "metadata": {},
   "source": [
    "## 7. TabICL Two-Stage + Do-Loss training + evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be3ae8-a796-4fc3-9f81-b8938738f6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d75b4e1a-6f3f-4e2f-9f84-96d926a5aa76",
   "metadata": {},
   "source": [
    "## 8. Guardado de salidas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-debugging]",
   "language": "python",
   "name": "conda-env-anaconda3-debugging-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
